{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask learning notebook\n",
    "\n",
    "In this notebook we'll do some toy experiments with a multitask learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as opt\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from mlearn.utils.metrics import Metrics\n",
    "from mlearn.utils.train import train_mtl_model\n",
    "from mlearn.data.dataset import GeneralDataset\n",
    "from mlearn.data.clean import Preprocessors, Cleaner\n",
    "from mlearn.data.loaders import hoover, davidson, garcia, semeval_sentiment\n",
    "from mlearn.modeling.multitask import EmbeddingLSTMClassifier, OnehotLSTMClassifier, OnehotMLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Hoover et al. (train): 999it [00:04, 239.64it/s]\n",
      "Building vocabulary: 100%|██████████| 799/799 [00:00<00:00, 90814.83it/s]\n",
      "Encoding vocabulary: 100%|██████████| 2843/2843 [00:00<00:00, 563988.38it/s]\n",
      "Encode label vocab: 100%|██████████| 11/11 [00:00<00:00, 13462.90it/s]\n",
      "Loading Davidson et al. (train): 887it [00:04, 187.80it/s]\n",
      "Building vocabulary: 100%|██████████| 709/709 [00:00<00:00, 171963.31it/s]\n",
      "Encoding vocabulary: 100%|██████████| 2932/2932 [00:00<00:00, 301281.28it/s]\n",
      "Encode label vocab: 100%|██████████| 3/3 [00:00<00:00, 2270.87it/s]\n",
      "Loading Semeval (2016) (train): 999it [00:04, 218.00it/s]\n",
      "Building vocabulary: 100%|██████████| 799/799 [00:00<00:00, 150475.91it/s]\n",
      "Encoding vocabulary: 100%|██████████| 3432/3432 [00:00<00:00, 674263.49it/s]\n",
      "Encode label vocab: 100%|██████████| 3/3 [00:00<00:00, 8224.13it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner(['lower', 'hashtag', 'url', 'user'])\n",
    "data_path = '../tests/data/'\n",
    "\n",
    "# Hoover dataset\n",
    "mftc = hoover(cleaners = cleaner, data_path = data_path)\n",
    "mftc.build_token_vocab(mftc.data)\n",
    "mftc.build_label_vocab(mftc.data)\n",
    "\n",
    "# Davidson dataset\n",
    "off = davidson(cleaners = cleaner, data_path = data_path)\n",
    "off.build_token_vocab(off.data)\n",
    "off.build_label_vocab(off.data)\n",
    "\n",
    "# Sentiment analysis dataset\n",
    "sent = semeval_sentiment(cleaners = cleaner, data_path = data_path)\n",
    "sent.build_token_vocab(sent.data)\n",
    "sent.build_label_vocab(sent.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches of train and dev"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "onehot_mftc_train = process_and_batch(mftc, mftc.data, batch_size = 32, onehot = True)\n",
    "onehot_mftc_dev = process_and_batch(mftc, mftc.dev, batch_size = 32, onehot = True)\n",
    "onehot_davidson_train = process_and_batch(off, off.data, batch_size = 32, onehot = True)\n",
    "onehot_davidson_dev = process_and_batch(off, off.dev, batch_size = 32, onehot = True)\n",
    "onehot_sent_train = process_and_batch(sent, sent.data, batch_size = 32, onehot = True)\n",
    "onehot_sent_dev = process_and_batch(sent, sent.dev, batch_size = 32, onehot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3434, 2934, 2845]\n"
     ]
    }
   ],
   "source": [
    "input_dims = [sent.vocab_size(), off.vocab_size(), mftc.vocab_size()]\n",
    "output_dims = [sent.label_count(), off.label_count(), mftc.label_count()]\n",
    "shared_dim, hidden_dims, embedding_dim = 256, [64, 128, 300], 128\n",
    "dropout = 0.4\n",
    "print(input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Batch:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnehotMLPClassifier(\n",
      "  (all_parameters): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 64x3434]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 64]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 64x2934]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 64]\n",
      "      (4): Parameter containing: [torch.FloatTensor of size 64x2845]\n",
      "      (5): Parameter containing: [torch.FloatTensor of size 64]\n",
      "      (6): Parameter containing: [torch.FloatTensor of size 128x64]\n",
      "      (7): Parameter containing: [torch.FloatTensor of size 128]\n",
      "      (8): Parameter containing: [torch.FloatTensor of size 300x128]\n",
      "      (9): Parameter containing: [torch.FloatTensor of size 300]\n",
      "      (10): Parameter containing: [torch.FloatTensor of size 3x300]\n",
      "      (11): Parameter containing: [torch.FloatTensor of size 3]\n",
      "      (12): Parameter containing: [torch.FloatTensor of size 3x300]\n",
      "      (13): Parameter containing: [torch.FloatTensor of size 3]\n",
      "      (14): Parameter containing: [torch.FloatTensor of size 11x300]\n",
      "      (15): Parameter containing: [torch.FloatTensor of size 11]\n",
      "  )\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[-0.0107, -0.0025,  0.0165,  ..., -0.0012,  0.0054,  0.0100],\n",
      "        [ 0.0078, -0.0029,  0.0007,  ..., -0.0073, -0.0139,  0.0108],\n",
      "        [ 0.0164, -0.0079, -0.0165,  ...,  0.0064, -0.0129,  0.0146],\n",
      "        ...,\n",
      "        [-0.0101, -0.0152, -0.0152,  ...,  0.0094, -0.0078,  0.0108],\n",
      "        [ 0.0007,  0.0023, -0.0064,  ...,  0.0126,  0.0040,  0.0135],\n",
      "        [-0.0096,  0.0084, -0.0024,  ..., -0.0124, -0.0147,  0.0070]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-4.8947e-03, -1.4098e-05, -2.4453e-03,  3.7150e-03, -1.4773e-02,\n",
      "         6.6391e-03,  1.4989e-02, -1.4696e-02,  6.7847e-03, -9.9606e-03,\n",
      "         1.4787e-02, -7.8568e-03,  1.5008e-02,  9.7652e-03, -7.3179e-03,\n",
      "        -3.1650e-03, -7.7475e-03, -1.0440e-03, -5.7243e-03,  1.2829e-02,\n",
      "        -8.6735e-03, -1.5930e-02,  1.0442e-02,  1.1658e-02,  2.0741e-03,\n",
      "        -1.5149e-02, -1.1743e-02,  8.0798e-04,  1.4867e-02,  1.5525e-02,\n",
      "         9.3221e-03, -1.6125e-02, -2.4923e-04,  1.4476e-02,  8.7619e-03,\n",
      "         1.1745e-02, -1.4155e-02, -9.6428e-03, -1.5065e-02,  1.6689e-02,\n",
      "        -9.5699e-03,  2.2177e-03, -7.3743e-03,  1.0760e-02, -5.4818e-03,\n",
      "         6.6411e-03,  1.4968e-02, -1.3988e-02, -1.3321e-04, -9.5854e-04,\n",
      "        -8.5706e-03, -1.5787e-02,  3.6262e-03,  1.1252e-02,  5.4283e-04,\n",
      "         3.1436e-03, -1.3357e-02,  6.7881e-03, -3.2286e-03, -1.4434e-02,\n",
      "         1.5221e-02, -1.4363e-02,  7.1922e-03,  5.7897e-03],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0052, -0.0109,  0.0100,  ...,  0.0084,  0.0015,  0.0135],\n",
      "        [-0.0078,  0.0105, -0.0137,  ...,  0.0114,  0.0129,  0.0026],\n",
      "        [-0.0011,  0.0056,  0.0074,  ..., -0.0018,  0.0060,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0070, -0.0136,  0.0061,  ...,  0.0173, -0.0038, -0.0051],\n",
      "        [ 0.0112, -0.0151, -0.0095,  ...,  0.0153, -0.0065, -0.0149],\n",
      "        [-0.0151, -0.0134, -0.0042,  ..., -0.0180, -0.0008,  0.0075]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0015,  0.0011, -0.0154,  0.0118,  0.0029, -0.0121, -0.0101, -0.0012,\n",
      "        -0.0117,  0.0066, -0.0060, -0.0118,  0.0184, -0.0102, -0.0006,  0.0039,\n",
      "        -0.0002, -0.0096, -0.0124, -0.0180, -0.0132,  0.0066, -0.0152,  0.0024,\n",
      "        -0.0030,  0.0142,  0.0047,  0.0101,  0.0119,  0.0068,  0.0148, -0.0085,\n",
      "         0.0177, -0.0108,  0.0041,  0.0027,  0.0001,  0.0030, -0.0072, -0.0159,\n",
      "         0.0013, -0.0130,  0.0106, -0.0064, -0.0152, -0.0181, -0.0064, -0.0184,\n",
      "        -0.0049,  0.0034, -0.0068, -0.0075, -0.0001, -0.0091, -0.0163, -0.0164,\n",
      "        -0.0089,  0.0168,  0.0159, -0.0083,  0.0144,  0.0125, -0.0110,  0.0036],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0026,  0.0007,  0.0034,  ..., -0.0166, -0.0118, -0.0153],\n",
      "        [ 0.0117, -0.0185, -0.0039,  ..., -0.0179, -0.0101,  0.0056],\n",
      "        [-0.0150,  0.0141,  0.0022,  ...,  0.0112, -0.0011,  0.0169],\n",
      "        ...,\n",
      "        [-0.0158,  0.0181,  0.0117,  ..., -0.0045, -0.0068, -0.0161],\n",
      "        [ 0.0042, -0.0157, -0.0069,  ...,  0.0045, -0.0114, -0.0069],\n",
      "        [ 0.0138,  0.0070, -0.0139,  ...,  0.0147,  0.0126, -0.0077]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0088,  0.0007, -0.0101, -0.0115,  0.0102,  0.0058,  0.0183,  0.0033,\n",
      "         0.0084, -0.0167, -0.0034, -0.0092, -0.0172, -0.0127, -0.0169, -0.0050,\n",
      "        -0.0088, -0.0184,  0.0054, -0.0114,  0.0094, -0.0070,  0.0109,  0.0084,\n",
      "        -0.0056,  0.0138,  0.0031,  0.0095, -0.0004, -0.0153, -0.0142, -0.0095,\n",
      "         0.0103,  0.0010, -0.0129,  0.0013, -0.0163,  0.0154, -0.0085,  0.0178,\n",
      "        -0.0098,  0.0071, -0.0186, -0.0182,  0.0013,  0.0067,  0.0162, -0.0032,\n",
      "         0.0017,  0.0109,  0.0123,  0.0132,  0.0088, -0.0068,  0.0138,  0.0147,\n",
      "         0.0135, -0.0071, -0.0158, -0.0064,  0.0078,  0.0038,  0.0137,  0.0033],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0776, -0.0442, -0.0396,  ..., -0.0443, -0.0078, -0.0925],\n",
      "        [-0.0468,  0.0388, -0.0610,  ...,  0.0154, -0.0963,  0.0871],\n",
      "        [ 0.0362, -0.1070,  0.0845,  ...,  0.1247,  0.0309,  0.0225],\n",
      "        ...,\n",
      "        [ 0.0577,  0.0364, -0.0709,  ..., -0.0531, -0.0553,  0.0326],\n",
      "        [-0.1085, -0.1102, -0.0130,  ..., -0.1064,  0.0212, -0.0671],\n",
      "        [-0.0815,  0.0949, -0.0246,  ...,  0.0029, -0.0050, -0.1052]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 1.0690e-01, -7.3358e-02,  9.8600e-02, -9.3955e-02, -6.4093e-02,\n",
      "        -5.2269e-02,  3.9439e-02, -2.7746e-02, -3.4032e-02,  3.2213e-02,\n",
      "        -1.1911e-01, -2.6161e-02,  1.1362e-01,  5.6557e-02,  3.5274e-03,\n",
      "        -6.1467e-02,  1.4334e-02, -1.9034e-02,  4.5788e-02,  1.2194e-02,\n",
      "        -1.2066e-01,  5.4034e-03, -1.1840e-01, -3.5748e-02, -3.4470e-02,\n",
      "         1.0243e-01,  6.4111e-02, -7.8954e-02, -8.8502e-02, -5.9140e-02,\n",
      "         9.3334e-02, -9.6754e-02, -8.5752e-03, -1.0324e-01, -8.8737e-02,\n",
      "         5.5190e-02,  5.0330e-02,  4.2660e-03,  1.9170e-03,  1.0961e-01,\n",
      "         9.6197e-02,  1.3598e-02,  1.5822e-02, -5.9638e-03,  4.0709e-03,\n",
      "         4.4459e-02,  1.1784e-01, -9.0832e-02,  1.6224e-02,  2.8109e-02,\n",
      "         1.0900e-01, -8.1402e-02,  9.5564e-02, -1.1976e-02,  9.3641e-02,\n",
      "        -6.9259e-02, -2.3617e-02,  4.8853e-02, -8.0965e-02,  8.0225e-02,\n",
      "        -7.1403e-02,  3.8300e-02,  3.4166e-02, -1.2112e-01,  3.1476e-02,\n",
      "         9.5871e-02, -4.9636e-02,  1.0125e-01,  4.4098e-02, -8.9993e-02,\n",
      "         2.0533e-03,  7.1126e-03,  8.6540e-02, -8.5536e-04,  1.5408e-02,\n",
      "        -1.0110e-01,  1.0731e-01, -4.9233e-02,  6.4171e-02, -6.0526e-03,\n",
      "        -6.3364e-02,  4.9448e-02,  1.2266e-01,  5.6223e-02, -2.9927e-02,\n",
      "         2.1501e-03,  7.1504e-02, -8.8868e-02,  9.5162e-02, -8.7448e-02,\n",
      "        -9.0574e-02, -9.1673e-02, -1.2015e-01,  4.7287e-02,  6.1305e-02,\n",
      "         1.0277e-01,  2.5274e-03, -8.9574e-02, -2.4260e-02, -5.6739e-02,\n",
      "        -3.9274e-02,  3.6574e-03, -3.1402e-02,  6.4671e-06,  4.4914e-03,\n",
      "        -1.1006e-01, -1.9748e-02,  2.3298e-03,  6.7443e-02,  7.3534e-02,\n",
      "        -3.3972e-02,  9.7573e-02,  6.3551e-02,  7.1176e-03,  6.6370e-02,\n",
      "         7.1440e-02, -3.3246e-02,  1.0334e-01, -2.8495e-03,  2.5485e-02,\n",
      "         8.6147e-02,  2.7687e-02, -1.1873e-01,  4.4035e-02,  9.3280e-02,\n",
      "        -5.4326e-02, -4.2843e-02,  1.1285e-01], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0616, -0.0724,  0.0228,  ..., -0.0222,  0.0519,  0.0498],\n",
      "        [ 0.0403, -0.0256,  0.0064,  ...,  0.0571,  0.0261,  0.0434],\n",
      "        [-0.0831, -0.0501, -0.0775,  ..., -0.0657,  0.0651, -0.0569],\n",
      "        ...,\n",
      "        [ 0.0584,  0.0669,  0.0820,  ..., -0.0411, -0.0612, -0.0558],\n",
      "        [-0.0448, -0.0653, -0.0133,  ..., -0.0514,  0.0367,  0.0438],\n",
      "        [ 0.0148,  0.0351, -0.0271,  ...,  0.0114,  0.0862, -0.0814]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0472, -0.0815,  0.0423,  0.0826, -0.0214, -0.0060, -0.0496, -0.0404,\n",
      "        -0.0650, -0.0568,  0.0578, -0.0764,  0.0739, -0.0829,  0.0536,  0.0819,\n",
      "        -0.0883, -0.0874, -0.0355,  0.0460,  0.0694, -0.0758, -0.0732, -0.0254,\n",
      "         0.0665,  0.0741, -0.0698, -0.0006, -0.0638,  0.0481,  0.0666, -0.0553,\n",
      "        -0.0008,  0.0858,  0.0414, -0.0772, -0.0010, -0.0612,  0.0141,  0.0546,\n",
      "        -0.0719, -0.0653, -0.0697, -0.0290,  0.0614, -0.0433, -0.0625, -0.0220,\n",
      "        -0.0792,  0.0612, -0.0291,  0.0327, -0.0880, -0.0322,  0.0371, -0.0274,\n",
      "         0.0395,  0.0350,  0.0816,  0.0470, -0.0028,  0.0605, -0.0567, -0.0095,\n",
      "         0.0595, -0.0243,  0.0705,  0.0580,  0.0791, -0.0872,  0.0418, -0.0466,\n",
      "         0.0190, -0.0704, -0.0149, -0.0123,  0.0152, -0.0169,  0.0216,  0.0258,\n",
      "        -0.0158,  0.0192, -0.0848,  0.0057,  0.0579,  0.0422,  0.0700,  0.0417,\n",
      "         0.0009, -0.0491,  0.0733,  0.0368, -0.0542, -0.0234, -0.0842,  0.0696,\n",
      "         0.0132,  0.0342,  0.0868, -0.0046, -0.0268,  0.0651, -0.0238,  0.0001,\n",
      "         0.0362, -0.0105,  0.0512, -0.0439,  0.0689, -0.0470,  0.0036, -0.0355,\n",
      "        -0.0397, -0.0826,  0.0352,  0.0376,  0.0440, -0.0007, -0.0511, -0.0018,\n",
      "        -0.0735,  0.0220, -0.0720, -0.0737,  0.0129,  0.0566, -0.0335, -0.0831,\n",
      "         0.0046,  0.0340, -0.0800,  0.0838,  0.0465,  0.0834, -0.0823,  0.0133,\n",
      "        -0.0226,  0.0584, -0.0785, -0.0517,  0.0542,  0.0392, -0.0058,  0.0471,\n",
      "         0.0134, -0.0407, -0.0459,  0.0698, -0.0386,  0.0216, -0.0485,  0.0005,\n",
      "        -0.0553, -0.0005,  0.0366,  0.0513, -0.0207,  0.0572,  0.0015,  0.0845,\n",
      "        -0.0736, -0.0202,  0.0744, -0.0579,  0.0443, -0.0013, -0.0105,  0.0754,\n",
      "        -0.0054,  0.0064,  0.0502, -0.0239, -0.0235,  0.0083,  0.0573, -0.0215,\n",
      "         0.0720,  0.0622, -0.0072,  0.0279, -0.0847, -0.0840,  0.0576, -0.0169,\n",
      "         0.0014,  0.0342,  0.0093,  0.0741,  0.0399, -0.0442,  0.0670,  0.0406,\n",
      "        -0.0110,  0.0265,  0.0277,  0.0740, -0.0659, -0.0003,  0.0267, -0.0100,\n",
      "        -0.0530, -0.0783, -0.0163,  0.0499,  0.0210, -0.0247,  0.0094,  0.0713,\n",
      "        -0.0390,  0.0607, -0.0002,  0.0713, -0.0435,  0.0158,  0.0196, -0.0142,\n",
      "        -0.0344, -0.0128, -0.0540,  0.0482,  0.0590,  0.0096, -0.0691, -0.0350,\n",
      "        -0.0410, -0.0245,  0.0551, -0.0802,  0.0525, -0.0510,  0.0761,  0.0864,\n",
      "        -0.0787,  0.0710, -0.0222,  0.0813,  0.0525,  0.0109,  0.0501,  0.0570,\n",
      "        -0.0496, -0.0506,  0.0723, -0.0616, -0.0273,  0.0322,  0.0032, -0.0087,\n",
      "        -0.0429,  0.0122,  0.0161, -0.0531, -0.0835, -0.0187, -0.0800,  0.0059,\n",
      "         0.0496,  0.0506, -0.0333, -0.0690, -0.0129,  0.0490, -0.0408,  0.0749,\n",
      "        -0.0672,  0.0556,  0.0760,  0.0651, -0.0181, -0.0143, -0.0280,  0.0752,\n",
      "         0.0118, -0.0032, -0.0533,  0.0651, -0.0241,  0.0308, -0.0002, -0.0773,\n",
      "        -0.0730,  0.0617, -0.0707,  0.0068,  0.0791, -0.0274,  0.0437, -0.0634,\n",
      "        -0.0205,  0.0624, -0.0097, -0.0317,  0.0801, -0.0464,  0.0467, -0.0089,\n",
      "         0.0461, -0.0209, -0.0577,  0.0677], requires_grad=True), Parameter containing:\n",
      "tensor([[-2.7164e-02,  3.7897e-02, -4.3690e-02,  3.8024e-02, -1.8298e-02,\n",
      "         -1.9892e-02, -3.7549e-02, -4.1638e-02,  3.8898e-02,  4.0529e-02,\n",
      "         -5.5254e-02,  3.3795e-02,  7.3862e-03,  4.1831e-03, -5.1686e-02,\n",
      "         -2.9574e-02, -1.5037e-03, -4.4514e-02,  4.7664e-02, -2.2212e-02,\n",
      "          1.7331e-02, -5.6057e-02, -3.5255e-03, -3.8604e-02, -5.5506e-02,\n",
      "         -3.4934e-03, -3.3231e-02,  4.9224e-02,  2.9135e-02,  1.8273e-02,\n",
      "         -7.3698e-03,  2.1954e-02, -2.9602e-02, -5.6984e-02, -4.3169e-02,\n",
      "          4.8622e-02, -5.1844e-02,  2.2066e-02,  3.6054e-02, -3.1907e-02,\n",
      "         -2.1351e-02, -5.6217e-02,  2.4274e-02, -4.5896e-02,  4.1684e-02,\n",
      "          4.9694e-03,  5.5728e-02,  1.1693e-02, -3.9731e-02,  7.1437e-03,\n",
      "          3.0837e-02, -4.1397e-02,  6.1275e-03,  5.3910e-02,  9.8367e-03,\n",
      "         -5.7267e-02,  3.4122e-02,  3.9096e-02, -1.6086e-02,  5.6027e-02,\n",
      "         -3.5451e-02, -7.3691e-04,  1.4853e-02,  2.7451e-02,  3.0525e-03,\n",
      "          3.5208e-02,  1.8059e-02,  1.4568e-02, -5.2013e-02, -2.7645e-02,\n",
      "         -5.0296e-02,  5.3803e-02,  4.0451e-02,  5.7725e-02, -1.4276e-02,\n",
      "         -3.2048e-02,  4.0660e-02,  7.3718e-03,  1.5851e-02,  4.3759e-02,\n",
      "         -2.9389e-02,  1.9723e-02, -2.6614e-02, -5.6914e-02,  3.1587e-02,\n",
      "         -4.9559e-02, -3.3634e-02, -3.6279e-02,  1.8388e-02,  4.3638e-02,\n",
      "          3.2447e-02,  2.5846e-02,  2.0225e-02,  5.5172e-02,  3.9754e-02,\n",
      "          1.0142e-02,  2.7639e-02,  1.0194e-02, -5.1537e-02, -3.0751e-02,\n",
      "          2.3346e-02,  3.5878e-02, -1.1996e-03,  3.5360e-02, -1.4310e-02,\n",
      "         -3.4669e-02,  4.8168e-02,  5.0846e-03, -1.0934e-02,  5.2963e-02,\n",
      "          4.3825e-02, -4.0615e-02, -5.5421e-02, -5.3322e-02,  5.5802e-02,\n",
      "         -5.0751e-03,  7.8302e-03, -4.0922e-02,  5.1020e-02,  2.8039e-02,\n",
      "          3.4794e-02,  1.4743e-02,  1.3086e-02,  3.4390e-02, -3.4756e-02,\n",
      "          2.9204e-02,  5.7444e-02, -4.3326e-02,  4.6308e-02, -1.0018e-02,\n",
      "         -1.5225e-02,  5.1712e-02,  4.8467e-02, -1.6570e-02,  5.2275e-02,\n",
      "          3.9518e-02,  4.7472e-02,  2.5109e-03,  2.9763e-02,  4.8077e-02,\n",
      "          9.7983e-03, -2.0020e-02, -5.1108e-02, -4.2471e-02,  3.4680e-02,\n",
      "         -5.1847e-02,  3.5175e-03, -8.7406e-03, -4.3141e-02, -8.4599e-03,\n",
      "         -5.7596e-02,  4.5306e-02, -3.3830e-02,  1.0908e-02, -2.5884e-02,\n",
      "         -4.2145e-02, -4.5581e-02, -5.6547e-02,  1.0138e-02, -1.6396e-02,\n",
      "          1.9673e-02,  1.8795e-02, -8.1140e-03,  3.5022e-02, -3.9576e-03,\n",
      "          3.9344e-02,  8.4616e-04,  3.0758e-02, -5.7098e-02,  3.4719e-02,\n",
      "         -4.3748e-03,  4.4785e-02, -8.1302e-03,  5.1311e-02,  1.9235e-02,\n",
      "         -5.5862e-02, -3.1217e-02,  2.8115e-02,  4.7502e-02,  5.5153e-02,\n",
      "          4.3712e-02,  1.7547e-02, -1.1474e-02, -3.2932e-02, -3.2385e-02,\n",
      "          3.0851e-03, -6.2355e-03, -5.4568e-02,  5.1277e-03, -3.8443e-02,\n",
      "          3.3951e-03, -2.2042e-02, -4.2689e-02, -5.5799e-02,  1.2517e-03,\n",
      "         -1.8459e-02,  9.6110e-04, -4.2133e-02,  3.2860e-02,  3.5730e-03,\n",
      "          5.6718e-02, -2.0971e-02,  1.6146e-02, -7.6285e-03,  3.5598e-02,\n",
      "          3.9244e-02, -3.5469e-02, -1.4743e-02, -5.4110e-02,  1.0398e-02,\n",
      "          3.1758e-02, -4.8468e-02,  3.7438e-02, -1.9793e-02,  5.4365e-02,\n",
      "          1.4225e-02,  4.2122e-02,  3.4442e-02,  5.3716e-02, -1.2684e-02,\n",
      "          3.0908e-02,  3.5959e-02, -1.5231e-02,  4.4853e-02, -4.6699e-02,\n",
      "         -2.4014e-02, -4.6492e-02, -4.4692e-02, -1.4175e-03, -4.2844e-02,\n",
      "          3.2733e-02, -3.2050e-02, -2.7433e-02, -4.5540e-02, -1.7197e-02,\n",
      "          1.1592e-02, -1.6376e-03, -2.8967e-02,  3.6111e-03,  3.1800e-02,\n",
      "         -3.1425e-02,  4.9644e-03,  6.2908e-03, -3.3650e-02, -2.8892e-02,\n",
      "          4.7714e-02, -6.6751e-03, -2.8654e-02, -3.6762e-02, -2.4498e-02,\n",
      "          1.5126e-02,  4.0829e-02, -1.8851e-02,  4.3358e-02,  4.4731e-02,\n",
      "          4.2217e-02,  3.0721e-02, -5.6436e-02, -1.4109e-02, -8.5277e-03,\n",
      "         -3.7291e-02,  5.5950e-02, -5.6175e-02, -4.4893e-02, -8.6801e-03,\n",
      "         -1.8889e-02,  2.2919e-02,  3.8453e-02, -2.9872e-02, -4.2667e-02,\n",
      "         -3.8508e-02, -1.3617e-02,  5.1961e-02, -2.1648e-03,  5.3217e-02,\n",
      "         -1.9642e-02,  3.8761e-02, -2.6464e-02,  9.8454e-04,  1.6336e-02,\n",
      "         -5.1667e-02,  2.6077e-02, -4.0503e-02,  4.8783e-02,  1.8908e-02,\n",
      "         -2.6415e-02, -8.5716e-03,  4.8110e-02, -3.3977e-02,  2.0672e-02,\n",
      "         -5.2404e-02, -5.1733e-02, -3.2160e-03,  1.8520e-02, -1.3722e-03,\n",
      "         -1.8378e-02,  4.1137e-02, -5.4491e-02,  1.3286e-02,  3.5309e-02],\n",
      "        [-4.4785e-02, -1.9180e-02, -5.5411e-02,  2.7594e-02,  2.0539e-02,\n",
      "          5.5467e-02,  4.1052e-02, -1.8079e-02, -2.4111e-02,  4.1294e-02,\n",
      "          1.1063e-02,  4.3103e-03, -3.6649e-02,  4.4216e-02,  4.2755e-02,\n",
      "         -2.2903e-02,  3.7967e-02, -3.9003e-02, -4.8326e-02,  2.4116e-02,\n",
      "         -2.1291e-02, -4.0478e-02,  2.4380e-02,  2.2344e-02, -5.0909e-02,\n",
      "         -3.9020e-02, -2.3137e-02,  9.3151e-03, -5.6161e-02, -4.1667e-02,\n",
      "         -2.0472e-02,  4.4269e-03, -4.8895e-02,  1.5090e-02, -4.6455e-02,\n",
      "         -3.5947e-02, -5.5396e-02,  1.6493e-02, -5.5275e-02,  1.2944e-02,\n",
      "         -1.4977e-02,  1.4315e-03, -3.1903e-02,  6.5603e-03,  1.9111e-02,\n",
      "         -3.1098e-02, -4.9495e-02,  6.2814e-03,  5.2526e-02,  4.7528e-02,\n",
      "          2.2065e-02, -2.7988e-02,  5.6265e-02, -2.3336e-02, -2.6002e-02,\n",
      "          1.8563e-02,  2.2996e-02,  1.4388e-02, -1.7454e-02, -5.3519e-02,\n",
      "          3.2357e-02, -3.0182e-02, -2.5864e-02,  4.5334e-02,  2.4475e-02,\n",
      "         -3.1528e-02, -7.6897e-03,  5.7404e-02, -4.8715e-02, -4.7532e-02,\n",
      "          5.4619e-02,  5.6658e-02,  5.1718e-02, -5.0254e-02,  4.7184e-02,\n",
      "          4.2265e-02, -3.8791e-02, -5.3824e-02,  4.9293e-02,  3.6961e-02,\n",
      "         -5.2328e-02,  5.1126e-02, -3.7746e-02,  3.5732e-02,  2.7205e-02,\n",
      "          4.1839e-02,  4.6691e-02, -3.8049e-02,  1.9251e-02, -2.6519e-02,\n",
      "         -3.4011e-02, -1.6544e-02, -4.5121e-02, -9.3156e-03, -3.3787e-02,\n",
      "         -6.4094e-03, -1.4974e-02,  6.3207e-03, -4.2062e-02,  2.6373e-02,\n",
      "          3.3440e-02,  4.4684e-02,  1.2372e-02,  2.3763e-02, -2.8225e-02,\n",
      "         -3.1519e-02, -5.7700e-02, -1.9496e-02,  2.7220e-02,  2.5165e-02,\n",
      "          2.8894e-02,  5.0708e-02,  1.8653e-02,  3.1920e-02,  2.0758e-02,\n",
      "         -3.7603e-02, -4.1622e-02,  1.1432e-02,  1.2435e-02,  6.3362e-03,\n",
      "          6.3001e-04,  3.5466e-02, -3.1703e-02,  1.2590e-02, -5.2565e-02,\n",
      "         -1.6519e-02,  1.2907e-02,  6.2715e-03, -2.3070e-02,  4.4692e-02,\n",
      "          5.4190e-02, -1.0861e-02, -7.1123e-03,  3.8021e-02,  2.4309e-02,\n",
      "          3.6464e-02, -3.5581e-02,  6.4425e-03, -1.5765e-02,  2.4258e-02,\n",
      "          2.2971e-02,  3.7100e-02,  1.4070e-02, -5.1950e-02,  4.1290e-02,\n",
      "         -1.8265e-02,  4.5885e-02,  5.8966e-03,  3.5740e-02,  1.5516e-02,\n",
      "         -3.4867e-02, -4.3183e-02, -2.8667e-02,  5.0119e-03, -1.5361e-02,\n",
      "         -1.9256e-02,  1.2169e-02, -4.9713e-02,  4.8608e-02, -5.2756e-02,\n",
      "          5.8920e-03, -5.3955e-02,  5.6086e-02,  2.2552e-02,  1.1621e-02,\n",
      "          1.1619e-02,  5.3436e-02,  5.6186e-03,  3.4891e-02,  5.2744e-02,\n",
      "         -1.3438e-02,  2.3121e-02,  3.0344e-02, -1.8874e-04,  5.5686e-03,\n",
      "          4.4773e-02,  4.7463e-02,  1.3483e-02,  7.6690e-03, -1.7213e-02,\n",
      "          3.0145e-02, -1.5060e-02,  3.1118e-02,  8.8451e-03,  2.2864e-02,\n",
      "         -1.8433e-02,  4.6235e-02,  1.9434e-02,  1.9757e-03, -2.8254e-02,\n",
      "         -2.6757e-02,  4.1025e-02, -8.5577e-03,  2.1234e-02,  4.6216e-02,\n",
      "          3.1336e-03,  4.2444e-02,  5.1218e-02,  4.8504e-02, -5.4916e-02,\n",
      "          4.1609e-02,  1.8038e-02,  2.5779e-02,  2.7120e-02, -4.7639e-02,\n",
      "         -1.2756e-02,  2.3074e-02,  3.6329e-03,  1.2330e-02, -1.5456e-02,\n",
      "         -2.7075e-02, -4.5419e-02,  1.8139e-02, -4.6536e-02, -3.2359e-02,\n",
      "          2.4877e-02,  3.4267e-02,  2.7702e-03, -4.2589e-02,  1.8424e-02,\n",
      "          1.2345e-02, -1.9068e-02,  9.1271e-03,  3.8903e-02,  4.5188e-02,\n",
      "         -1.0656e-02,  2.2107e-02,  3.0130e-02, -1.8919e-02, -2.9097e-02,\n",
      "         -4.6880e-02,  3.3363e-03,  5.8179e-03,  5.1910e-02, -4.0231e-02,\n",
      "         -4.5833e-02,  1.1960e-02,  5.9499e-03,  1.7044e-02, -1.9156e-02,\n",
      "          5.1219e-02,  5.3733e-02, -5.5924e-02,  4.3528e-03, -2.0697e-02,\n",
      "         -4.6163e-02, -2.1906e-02,  4.9243e-02, -2.3298e-02, -5.6571e-02,\n",
      "         -2.9789e-02, -4.8081e-02,  5.7021e-02, -4.5609e-02, -1.7691e-02,\n",
      "         -4.8375e-02, -4.7702e-02, -8.0780e-03,  5.6168e-02, -5.1953e-02,\n",
      "         -5.1502e-02,  4.8894e-02, -6.3631e-03,  3.0245e-02,  2.4109e-02,\n",
      "         -8.8580e-03, -4.1647e-03, -5.3520e-02,  5.4059e-02,  3.4088e-02,\n",
      "         -2.7671e-02, -4.2745e-02, -5.6338e-02,  3.7962e-02, -1.8184e-02,\n",
      "          1.9616e-02, -4.1404e-02, -3.8203e-02,  1.2634e-02,  3.7112e-02,\n",
      "         -4.9679e-02,  2.8686e-02, -4.3317e-02, -2.9598e-02, -4.9544e-02,\n",
      "          5.4460e-02,  4.7455e-02,  5.1722e-02,  2.5266e-02,  2.1678e-02,\n",
      "         -4.2800e-02, -4.8295e-02,  4.0764e-02, -3.8311e-02, -3.4624e-02,\n",
      "          5.0945e-02,  5.0786e-03, -1.0549e-02,  4.2767e-02,  2.2067e-02],\n",
      "        [-4.3530e-02,  2.5413e-02, -4.1016e-02, -5.4550e-02, -4.7720e-02,\n",
      "          3.7338e-02, -2.1732e-02, -1.2157e-02,  1.5869e-02,  3.9352e-02,\n",
      "         -4.3003e-02, -4.7905e-02,  5.5028e-02, -1.0918e-02, -4.3942e-02,\n",
      "         -3.2383e-02, -5.6300e-02, -5.5373e-02, -2.7912e-02,  1.0336e-02,\n",
      "          3.6394e-02, -5.0643e-02, -5.0070e-02, -2.9858e-02,  7.3267e-03,\n",
      "         -5.7516e-02, -5.3154e-03,  5.3668e-02,  2.8900e-02,  3.0378e-02,\n",
      "          4.0034e-02, -1.7597e-02, -1.0085e-02, -2.9289e-02,  5.9929e-03,\n",
      "         -2.8840e-02, -2.1620e-02, -3.2963e-02,  1.3338e-02, -2.4473e-02,\n",
      "          4.2665e-02,  3.4129e-02,  4.6693e-02, -5.0913e-02, -4.2993e-03,\n",
      "          4.7755e-02, -5.5102e-02, -4.9561e-02,  3.0253e-02,  3.9658e-02,\n",
      "         -1.9858e-02,  2.2702e-02,  3.6301e-02,  5.4559e-03, -1.2142e-02,\n",
      "          3.3005e-02,  5.2072e-02,  3.3339e-02,  1.4145e-02,  4.2711e-02,\n",
      "          3.3054e-03, -6.4448e-03, -1.6050e-02, -3.8432e-02, -1.8690e-02,\n",
      "         -1.6555e-03, -2.5804e-02, -2.3500e-02,  4.7309e-02,  4.1335e-02,\n",
      "          2.2351e-03, -5.4547e-02,  2.2483e-02,  6.0667e-03,  4.6260e-02,\n",
      "         -2.6726e-02, -1.0508e-02,  3.8670e-02,  1.8787e-02, -3.5203e-02,\n",
      "         -1.7906e-03,  4.8274e-02,  5.4431e-02,  5.0366e-02, -5.2440e-02,\n",
      "         -3.3204e-02,  5.2658e-02, -4.7884e-02,  9.4047e-03, -2.9641e-02,\n",
      "          4.2380e-02,  4.3942e-03,  4.7661e-02,  7.6820e-03,  4.2805e-02,\n",
      "         -2.9719e-02,  9.6858e-03, -8.1530e-04, -1.5461e-02, -4.7981e-02,\n",
      "          3.7070e-02,  9.7038e-03,  1.0134e-02, -3.8844e-02, -4.7330e-03,\n",
      "          4.9931e-02,  1.6882e-02, -3.2724e-02,  1.0640e-02, -5.0821e-02,\n",
      "          1.5781e-02,  5.8484e-03,  6.1408e-04,  3.4498e-02,  1.3438e-02,\n",
      "          1.2418e-02,  5.0392e-03,  5.5572e-02,  2.7192e-02,  4.0654e-03,\n",
      "         -3.9025e-03,  5.2485e-02, -3.9932e-02,  3.4508e-02,  8.5598e-03,\n",
      "         -9.3709e-03, -2.5767e-02, -3.0229e-02, -3.6615e-02, -4.8947e-02,\n",
      "          2.8789e-02, -1.6312e-03,  1.4809e-02, -4.3029e-02,  3.6247e-03,\n",
      "          4.0002e-02, -4.3076e-02, -5.2008e-02, -5.2033e-02,  3.5955e-02,\n",
      "          5.1919e-02,  3.0043e-02, -2.0944e-02,  6.3155e-03, -3.5674e-02,\n",
      "          2.2971e-02, -2.5632e-02, -3.0361e-02, -1.4892e-02,  5.3147e-02,\n",
      "          4.1012e-02,  3.9589e-02, -1.8453e-02,  5.2236e-02,  5.0936e-02,\n",
      "          3.9361e-02,  1.0772e-02,  4.4552e-03, -8.3904e-03, -4.1308e-02,\n",
      "         -2.0122e-02,  1.5417e-02, -1.4874e-02,  2.7616e-02, -4.9514e-02,\n",
      "         -4.4783e-02,  7.3341e-03,  2.9285e-02,  2.7319e-02,  2.9842e-02,\n",
      "         -2.6890e-03, -2.0883e-02,  1.9283e-03,  4.2423e-02,  8.7726e-03,\n",
      "         -5.7455e-02, -5.0740e-02,  5.0750e-02, -2.4701e-02,  4.6495e-02,\n",
      "          5.6597e-02,  4.7473e-02, -2.5781e-02, -2.7296e-03, -4.9539e-03,\n",
      "         -3.9923e-02,  5.5667e-02,  3.7930e-02,  3.3467e-03, -1.2543e-02,\n",
      "          8.8704e-03,  2.5902e-03,  1.9514e-02, -1.2283e-02,  4.1788e-02,\n",
      "          2.8633e-05,  1.7616e-03,  1.5912e-02,  5.7059e-02, -1.8712e-02,\n",
      "         -4.7340e-03, -2.2333e-02,  3.6431e-02,  3.7339e-02, -9.0053e-03,\n",
      "          5.1426e-02, -3.2661e-02, -5.2583e-02, -3.4382e-02,  4.5524e-02,\n",
      "          3.2600e-02,  4.6964e-02, -4.9487e-02,  8.0175e-03, -7.3773e-03,\n",
      "         -6.8842e-03,  4.4396e-02,  2.2114e-02, -3.4402e-02, -2.1313e-02,\n",
      "          5.5327e-03, -5.5805e-02, -2.6353e-02,  7.8673e-03,  8.4634e-03,\n",
      "          1.3658e-02,  4.8820e-02,  2.8138e-03,  2.1445e-02, -2.6145e-02,\n",
      "          3.9917e-02, -1.6568e-02, -1.9272e-02, -2.3666e-03, -2.2507e-02,\n",
      "         -1.7582e-02,  1.4902e-02,  1.4214e-02, -5.6539e-02, -1.4341e-02,\n",
      "         -5.3026e-02,  3.9160e-02,  5.5461e-02,  4.9050e-02, -5.6730e-02,\n",
      "         -2.5676e-02,  3.9694e-02, -4.5235e-02,  2.4915e-02, -3.0798e-02,\n",
      "          5.9305e-03,  2.6425e-02, -2.9131e-02, -4.4101e-02, -4.1336e-03,\n",
      "          4.6994e-02, -2.1738e-02,  4.7337e-02, -2.1483e-02,  3.3301e-02,\n",
      "          2.8096e-02, -4.2640e-02,  3.0482e-02,  3.1770e-02, -3.7287e-02,\n",
      "          7.7538e-03, -4.9954e-02, -4.8259e-02,  1.2180e-02,  2.5104e-02,\n",
      "         -3.0063e-02, -3.8218e-02, -3.9975e-02,  2.4621e-02, -1.5587e-02,\n",
      "          1.8297e-02,  3.9697e-02,  1.1658e-02, -5.4807e-02,  1.8959e-02,\n",
      "          5.6150e-02,  9.7492e-03, -2.6389e-02,  2.6339e-02, -3.7384e-02,\n",
      "         -1.8409e-02,  1.1355e-02,  1.3142e-02,  5.0830e-02,  1.5224e-03,\n",
      "          3.5892e-02,  1.8736e-02,  4.9257e-02,  2.5409e-02, -4.6082e-02,\n",
      "          4.9901e-02, -8.8139e-03,  4.0061e-02, -1.4749e-02,  1.6340e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0170, -0.0335, -0.0076], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.5535e-02,  3.4719e-02,  1.5878e-02,  2.7619e-02,  5.1615e-02,\n",
      "         -2.7510e-02,  4.3373e-02, -2.5101e-02,  3.2341e-02, -3.4596e-02,\n",
      "          4.4628e-02, -1.9757e-02,  6.4664e-03,  4.2939e-02, -3.5047e-02,\n",
      "         -1.5637e-02,  2.7529e-02,  2.5279e-02, -3.5043e-02, -1.3018e-02,\n",
      "          5.3278e-02, -4.0308e-02,  2.6316e-02, -2.2421e-02, -3.9922e-02,\n",
      "          4.8361e-02, -4.1451e-02,  3.9682e-03, -1.3026e-02, -2.0825e-03,\n",
      "          9.8361e-03,  5.7084e-02,  2.2196e-02, -1.1569e-02, -4.4652e-02,\n",
      "         -4.5848e-02, -4.1813e-04, -2.5198e-02, -2.5209e-02, -3.1928e-02,\n",
      "          3.1768e-02,  1.5136e-02, -3.1821e-02,  1.8155e-02, -4.1323e-02,\n",
      "         -1.5134e-02,  3.5394e-02, -2.1697e-02,  4.6576e-02,  7.8096e-03,\n",
      "         -2.1984e-02,  2.9467e-02,  2.7605e-02,  6.2475e-03,  2.8610e-02,\n",
      "         -4.9268e-02,  9.5154e-03,  4.1626e-02,  8.8316e-03, -2.0346e-02,\n",
      "          5.0738e-02, -3.6593e-02,  4.6396e-02, -5.3095e-02, -2.0746e-02,\n",
      "         -3.8507e-02,  5.1858e-02, -4.3687e-02,  4.5691e-02, -1.4184e-02,\n",
      "          4.6679e-02,  3.7771e-02, -3.0230e-02, -5.6631e-02,  4.2607e-02,\n",
      "          1.3794e-02, -2.2215e-02,  5.7311e-02, -3.6023e-02, -4.2875e-03,\n",
      "         -3.4674e-02,  1.1820e-02,  1.1879e-02,  9.6290e-03, -3.2278e-02,\n",
      "         -1.1463e-02,  1.9703e-02,  5.4529e-02, -2.0767e-02,  4.6484e-02,\n",
      "         -1.2062e-02, -2.3655e-03,  2.6044e-02, -5.6239e-04, -1.5113e-02,\n",
      "          6.3379e-03, -4.0706e-02,  3.6079e-02, -3.9392e-02,  7.5863e-03,\n",
      "          1.5584e-02,  1.8114e-02,  3.5039e-02,  6.6280e-03,  5.8860e-03,\n",
      "         -1.9193e-02,  4.8774e-03,  1.7903e-02, -5.2101e-02, -1.9418e-02,\n",
      "          2.0367e-02,  5.5049e-02,  1.0891e-02, -8.9771e-03, -2.0883e-02,\n",
      "         -4.2241e-03,  5.4446e-02, -2.3931e-02,  1.6621e-02,  4.1171e-02,\n",
      "          4.2118e-02, -2.1239e-02, -2.4846e-02, -4.7497e-02,  4.9691e-02,\n",
      "         -5.1051e-02,  4.4895e-02, -2.8256e-02, -3.8744e-02, -6.3892e-03,\n",
      "         -2.1382e-02,  2.5549e-02, -5.2956e-02,  4.2906e-02, -9.2435e-03,\n",
      "          1.6874e-02,  4.3040e-02, -2.2881e-02, -1.3604e-02, -5.0463e-02,\n",
      "          3.1043e-02,  5.2365e-02,  1.6929e-02,  1.1194e-02, -8.4881e-03,\n",
      "          2.0173e-02, -3.4728e-02,  1.1665e-03, -3.2886e-02, -2.1929e-02,\n",
      "         -2.9234e-02, -3.3578e-02, -3.9077e-02,  5.4430e-02, -3.6346e-02,\n",
      "         -2.2970e-02,  1.2657e-02, -5.4408e-02,  1.1883e-02, -4.2636e-03,\n",
      "          4.1754e-02, -4.5072e-02,  2.7438e-02,  4.8897e-02,  4.4040e-02,\n",
      "         -4.6507e-02, -3.7173e-02,  3.5251e-02,  1.9954e-02, -2.9000e-02,\n",
      "          4.7537e-02, -5.6356e-02, -4.8948e-02, -1.3252e-02,  2.1619e-02,\n",
      "          6.2632e-03,  4.7588e-02, -7.2557e-03,  4.3590e-02, -1.9472e-03,\n",
      "          6.6075e-03,  1.1887e-02, -5.0876e-02,  3.3083e-02, -9.1748e-03,\n",
      "         -3.5814e-02, -4.7705e-02, -4.3959e-02, -8.9956e-03,  1.1458e-02,\n",
      "          5.7123e-02,  2.8980e-02,  1.0686e-02, -2.4743e-03,  3.9917e-02,\n",
      "         -1.2917e-03, -1.7776e-02,  2.8024e-02,  4.5563e-02,  2.3088e-02,\n",
      "         -9.7003e-03,  5.6632e-02,  1.6772e-02, -3.3441e-02,  7.6198e-03,\n",
      "          3.3276e-02, -5.5580e-02, -2.9639e-02,  2.3874e-02, -4.1886e-02,\n",
      "          4.4234e-02, -1.7445e-02,  5.3006e-02,  4.6821e-02, -5.6532e-02,\n",
      "         -5.1954e-02,  5.0897e-02, -2.2800e-02, -4.3845e-02,  5.3283e-02,\n",
      "         -3.1759e-02,  3.4412e-02, -5.5401e-02,  5.0839e-02, -4.5848e-02,\n",
      "          1.5223e-02, -4.6357e-02, -3.3843e-02, -8.0654e-03,  5.7480e-02,\n",
      "         -1.5811e-02,  1.6856e-03,  3.9276e-02,  5.7518e-02,  9.8454e-03,\n",
      "          4.3158e-02,  3.7848e-02,  1.8805e-02,  5.0533e-02,  4.7225e-02,\n",
      "         -5.6520e-02,  5.4681e-02,  5.3261e-02, -4.3492e-02, -9.4721e-04,\n",
      "         -2.0502e-02,  4.9597e-02, -5.6075e-02,  1.1157e-02,  3.6868e-03,\n",
      "          8.8924e-03, -4.4407e-02, -3.2628e-02,  7.7655e-03,  5.6858e-02,\n",
      "          2.1576e-02,  5.3744e-02, -4.9566e-02, -3.3128e-02, -4.4739e-02,\n",
      "          5.1633e-02, -2.2511e-02, -3.2341e-02,  3.5082e-02,  2.5855e-02,\n",
      "          4.9780e-02, -4.0711e-02,  3.8383e-02, -6.8893e-03,  3.8712e-03,\n",
      "         -9.6716e-03, -1.1789e-03,  1.9917e-02,  4.7082e-02, -1.1291e-02,\n",
      "          3.5715e-02,  2.4496e-02, -5.2327e-02, -3.3693e-02,  5.3663e-02,\n",
      "         -3.2796e-02,  5.2614e-03, -3.2469e-02,  5.0522e-02,  5.6138e-02,\n",
      "          3.9819e-02, -2.7937e-02, -4.6938e-02, -3.9362e-02,  1.6091e-02,\n",
      "          2.1160e-02,  1.4134e-02,  2.0632e-02, -3.1265e-02,  3.5640e-02,\n",
      "          4.1526e-02,  2.2997e-02,  9.9862e-03, -1.8978e-02,  9.2751e-03],\n",
      "        [ 2.1721e-02,  2.0095e-02,  4.6323e-02,  2.3661e-02, -1.9122e-02,\n",
      "          2.7699e-02,  8.9667e-03, -3.3153e-02, -3.0768e-02,  2.1192e-03,\n",
      "         -5.3841e-02, -5.7017e-02, -9.6954e-03,  4.6857e-02,  2.9344e-02,\n",
      "         -2.1041e-02, -2.4856e-02, -3.1021e-02,  4.2428e-02, -8.5634e-03,\n",
      "         -1.2522e-02,  5.5254e-03,  3.1977e-02,  5.0109e-03, -2.4550e-02,\n",
      "         -2.7961e-02,  1.6402e-02,  2.6732e-02, -2.3002e-02, -5.0701e-03,\n",
      "         -3.2970e-02, -1.0386e-02,  3.9022e-02, -3.5781e-03, -5.6171e-04,\n",
      "         -2.3024e-02, -4.9906e-02,  4.2323e-02,  1.2629e-02, -1.5061e-02,\n",
      "          4.1957e-02, -2.1864e-02,  1.6890e-02, -1.1364e-02, -2.1675e-02,\n",
      "         -5.2534e-02, -2.0505e-02, -1.8124e-02,  5.0675e-02,  3.5424e-02,\n",
      "         -4.7413e-04, -5.3357e-02,  2.3368e-02, -2.1194e-02, -2.2080e-02,\n",
      "          1.5679e-02,  3.7908e-02, -5.3967e-03, -1.4282e-02, -3.5664e-02,\n",
      "         -3.6090e-02, -2.9763e-02, -4.6531e-02,  2.5499e-02, -5.0990e-02,\n",
      "         -1.5077e-02, -1.4708e-02, -1.8764e-03, -3.8632e-02,  9.6298e-03,\n",
      "          3.3912e-02, -5.5964e-02,  6.3595e-03,  6.4747e-03,  2.7978e-02,\n",
      "          1.7997e-02,  5.1655e-02,  4.2773e-02,  1.7338e-02,  3.2983e-02,\n",
      "          4.6938e-03,  1.2899e-02,  2.5325e-02,  2.9623e-02, -1.1161e-02,\n",
      "          3.2438e-02,  1.0053e-02, -4.9252e-02, -2.4050e-02, -1.0808e-02,\n",
      "         -2.5393e-02, -4.4439e-02,  1.0364e-02,  1.6135e-02, -7.4347e-03,\n",
      "         -2.9792e-02,  3.9793e-02, -3.1304e-02, -5.5656e-02,  2.7075e-03,\n",
      "         -3.1488e-03, -3.9450e-02,  5.0135e-02, -3.0614e-02,  5.4214e-02,\n",
      "          2.5608e-02,  2.6335e-02,  2.2976e-02,  1.0464e-02, -3.5051e-02,\n",
      "          4.5277e-02, -3.8162e-02, -3.7266e-03, -4.0656e-02,  5.5550e-03,\n",
      "          5.5568e-02,  2.1991e-02,  1.1779e-02, -4.6388e-02,  3.8080e-02,\n",
      "         -1.5366e-02, -4.5174e-02, -4.0076e-02, -5.4070e-03, -4.6377e-03,\n",
      "         -1.1567e-02, -4.4732e-03,  5.6442e-02,  3.7522e-02, -3.7494e-02,\n",
      "          3.2447e-02,  3.6028e-02, -8.1984e-03,  7.9198e-03, -5.6275e-02,\n",
      "          9.1472e-04, -2.5366e-02,  5.6911e-02,  2.2028e-02, -1.4189e-02,\n",
      "          1.8774e-02, -5.2148e-02, -8.0816e-03,  1.0547e-02,  4.9762e-02,\n",
      "         -1.5456e-03, -2.5016e-02,  3.9818e-02, -8.7698e-03,  5.6856e-02,\n",
      "         -5.6056e-02, -2.5282e-02,  2.9286e-02, -2.9486e-04,  3.5171e-03,\n",
      "          7.6095e-03,  3.2007e-02,  5.2524e-02, -1.3994e-02, -4.3384e-02,\n",
      "          4.7482e-02, -4.0709e-02,  2.8772e-02, -2.2808e-02, -2.2698e-02,\n",
      "          2.8664e-02, -2.0892e-02, -1.5977e-02,  3.3789e-02, -4.8597e-03,\n",
      "         -2.2449e-02, -1.9160e-02,  1.3642e-05, -2.1921e-02, -4.1271e-02,\n",
      "          1.2904e-02, -4.6989e-02,  1.0785e-02, -2.7452e-02, -2.9294e-02,\n",
      "          1.4495e-02,  2.9785e-02,  3.3834e-02,  1.4312e-02, -4.9718e-02,\n",
      "         -3.4538e-02, -5.1482e-02,  8.7684e-03,  5.5285e-02,  4.6938e-02,\n",
      "         -5.1406e-02,  4.2011e-02,  3.2740e-02, -5.3136e-02, -2.4494e-02,\n",
      "          4.8735e-02, -1.8046e-02, -2.8434e-03,  5.4189e-02, -2.3647e-02,\n",
      "         -3.5201e-03,  3.0909e-02, -2.5253e-03,  9.2872e-03, -1.4301e-02,\n",
      "          4.5906e-02, -4.7096e-02,  3.8567e-02,  4.7252e-02,  4.1876e-02,\n",
      "          4.6809e-02,  3.2162e-02, -5.5879e-02, -4.5616e-02, -4.5280e-02,\n",
      "         -2.5504e-02,  1.9890e-02, -3.9355e-02, -4.5276e-02, -1.7398e-02,\n",
      "          3.0273e-02,  4.9377e-02, -4.3307e-02,  3.8091e-02,  3.0303e-02,\n",
      "          2.5592e-02, -4.0628e-02, -1.8616e-02,  5.2689e-02, -2.6421e-02,\n",
      "          2.8265e-02,  2.2094e-02,  5.3761e-02, -3.7077e-02, -5.0099e-02,\n",
      "         -1.3210e-03,  2.8025e-02, -5.4381e-02,  5.6026e-02,  2.7543e-02,\n",
      "          5.4829e-02,  1.2296e-02,  3.1917e-02,  7.4033e-03, -1.6404e-02,\n",
      "         -5.0551e-02, -3.7813e-03, -4.3526e-02,  5.7590e-02, -3.3961e-02,\n",
      "          2.8836e-02, -3.5702e-02,  4.7965e-02,  1.5942e-02,  2.8907e-02,\n",
      "          5.5232e-02,  3.0832e-02,  1.0951e-03, -2.2379e-02,  5.5384e-02,\n",
      "          2.7974e-02, -1.8458e-02,  2.8069e-02, -4.7204e-02,  3.8261e-02,\n",
      "          5.0809e-02,  5.1917e-02,  4.8080e-02,  2.3324e-02,  2.1027e-02,\n",
      "         -9.0791e-04,  2.7252e-02, -5.4325e-02, -2.5880e-02, -4.2178e-02,\n",
      "          4.3634e-02,  5.1976e-02, -1.8954e-02, -2.6285e-02, -1.3967e-02,\n",
      "         -1.4840e-02,  2.0986e-02,  2.0403e-02,  3.8617e-02, -2.1581e-02,\n",
      "         -5.2058e-02,  8.2445e-03, -4.7614e-02,  3.1795e-02, -1.6318e-02,\n",
      "          5.0817e-02,  3.0960e-02,  1.7362e-02,  3.4498e-02, -1.0565e-02,\n",
      "          2.3104e-02,  1.4489e-02,  3.3929e-02, -1.9615e-02, -3.5852e-02],\n",
      "        [-7.0661e-03, -3.6231e-02,  1.7412e-02,  5.4255e-02, -2.9785e-02,\n",
      "          4.8757e-02, -4.2977e-02, -8.1607e-03,  4.8696e-02, -5.4975e-02,\n",
      "         -2.4930e-03, -2.8621e-02,  4.1935e-02, -4.9657e-02, -5.3893e-02,\n",
      "          1.8922e-02, -3.3454e-03,  8.3041e-03, -1.5770e-02,  4.2747e-02,\n",
      "         -1.4854e-04, -3.3654e-02, -4.7932e-02,  7.6640e-03,  4.9165e-02,\n",
      "         -4.7522e-02,  1.0019e-02, -1.2460e-02, -4.6972e-02,  5.5298e-02,\n",
      "         -2.3739e-02,  5.6261e-03, -2.4937e-02, -1.9236e-02, -1.8521e-02,\n",
      "          4.3910e-02,  5.7248e-02,  1.9382e-02,  4.3721e-02, -1.4289e-02,\n",
      "          5.2077e-02, -2.7288e-02,  3.5018e-02,  3.3885e-02, -2.5104e-02,\n",
      "          3.0349e-02,  5.5088e-02, -3.9735e-02, -1.4771e-02,  2.9005e-03,\n",
      "         -2.5789e-02,  8.9929e-03,  1.8946e-02, -5.2499e-03,  5.0782e-02,\n",
      "         -1.9471e-02,  1.4844e-03, -4.7174e-02, -7.4685e-03, -5.2751e-02,\n",
      "          2.6606e-02,  5.3857e-02, -4.9523e-03, -3.6563e-03, -1.0153e-03,\n",
      "          2.8754e-02,  5.6120e-02,  1.3405e-02,  2.4683e-02,  1.6293e-02,\n",
      "         -1.9593e-02,  5.6321e-02, -1.2124e-02,  1.1029e-02, -5.4611e-02,\n",
      "         -3.1766e-02, -2.2456e-02, -2.8377e-02,  3.1725e-02, -3.5403e-03,\n",
      "          1.6592e-02, -1.7582e-02, -4.5341e-02, -2.3870e-02,  5.5219e-02,\n",
      "          1.0173e-02,  4.8683e-02,  6.8783e-03, -7.8135e-03,  2.1923e-02,\n",
      "          2.2357e-02, -2.2852e-02, -2.1058e-02, -5.3855e-02, -1.9449e-02,\n",
      "          4.9392e-02, -3.4548e-02, -4.7959e-02, -1.5759e-02, -5.7098e-02,\n",
      "         -4.0373e-02,  3.0646e-02,  4.7106e-02,  8.5048e-03, -1.8019e-02,\n",
      "         -7.2335e-03, -3.7101e-02,  2.6774e-02,  1.6052e-02, -2.4405e-02,\n",
      "          2.9018e-02, -4.2517e-02, -5.6132e-02,  3.2673e-02, -2.6178e-02,\n",
      "         -1.5066e-02, -3.8012e-02, -9.5407e-03, -5.2927e-02,  4.4238e-02,\n",
      "         -3.0028e-02, -1.3286e-02,  5.2874e-02, -4.7875e-02, -3.0184e-03,\n",
      "         -1.0935e-02,  1.5410e-02,  5.1188e-02, -5.5184e-02,  1.5262e-02,\n",
      "          3.9087e-02,  2.7330e-02,  3.7073e-02, -7.8036e-03,  9.7740e-03,\n",
      "         -4.8955e-03, -5.3251e-02,  3.1917e-02,  2.7941e-02, -4.7350e-02,\n",
      "         -3.9289e-02, -1.7205e-02, -3.5357e-02, -4.9563e-02,  5.5993e-02,\n",
      "         -2.3788e-03, -3.2175e-02,  7.1321e-03,  3.0704e-02,  1.6116e-02,\n",
      "         -1.9113e-02,  2.2738e-02,  5.4912e-03,  5.2724e-02,  4.8649e-02,\n",
      "          1.4011e-02,  5.0741e-03, -4.2114e-02,  5.0138e-04, -4.2112e-02,\n",
      "          3.5749e-02, -3.5721e-02, -5.6885e-02,  1.2571e-02, -5.0638e-02,\n",
      "         -5.1844e-02,  2.1931e-02, -2.6810e-03, -2.3664e-02, -5.4268e-02,\n",
      "          7.8749e-03, -1.0184e-02,  3.9941e-02,  3.4192e-02,  2.8003e-02,\n",
      "          1.6819e-02,  4.8934e-02, -8.3038e-03,  5.3503e-02,  5.2536e-03,\n",
      "          3.4448e-02,  2.3472e-02, -3.8961e-02, -3.1089e-02,  1.3557e-02,\n",
      "         -4.7991e-02,  6.1491e-03,  2.8715e-02, -3.1816e-02,  1.1415e-02,\n",
      "          1.3307e-02, -4.5550e-02,  2.0645e-02,  4.0811e-02,  9.0353e-03,\n",
      "          2.5229e-02, -1.1353e-02, -2.1740e-02, -2.1702e-02, -1.1075e-02,\n",
      "          2.0635e-02, -5.1534e-02, -5.1782e-02, -2.9429e-02, -4.2598e-02,\n",
      "         -2.3717e-02,  6.2418e-03,  2.9559e-02,  1.2524e-02,  1.3964e-02,\n",
      "         -2.2577e-02,  4.0089e-02,  3.8492e-02,  6.9408e-03, -2.8984e-02,\n",
      "          4.5512e-02, -4.7475e-02,  1.8907e-03, -4.2929e-02, -3.7284e-02,\n",
      "          4.8814e-02,  4.2009e-02, -1.2617e-02, -3.4257e-02,  6.8465e-03,\n",
      "          2.2948e-02, -1.5478e-02, -5.5288e-02,  1.6545e-02,  4.5102e-02,\n",
      "          1.6427e-02,  8.3570e-04, -1.5202e-02, -4.0802e-02,  2.7996e-02,\n",
      "          1.5616e-02, -1.0208e-02,  2.5550e-02,  2.2106e-03, -5.6367e-02,\n",
      "         -1.1963e-02,  4.4301e-02,  3.7073e-02, -3.9157e-03,  1.7192e-02,\n",
      "          4.3721e-02,  5.2295e-02, -1.7009e-02,  2.2837e-03, -3.7050e-02,\n",
      "         -5.6955e-02, -5.3765e-02, -1.2056e-02, -3.5783e-03,  5.4650e-02,\n",
      "          3.1858e-02, -1.5058e-02,  5.6236e-02,  4.9606e-02, -5.5285e-02,\n",
      "         -3.6788e-02, -3.8621e-02, -1.0920e-02, -7.6333e-03, -5.6652e-02,\n",
      "          1.4624e-02, -5.4632e-02,  5.2271e-02, -8.7186e-03,  1.2181e-02,\n",
      "          2.4499e-02,  5.3268e-04,  1.0320e-03, -2.8935e-02, -4.0290e-02,\n",
      "         -5.1949e-02, -4.2651e-02, -1.3613e-02,  1.2078e-02,  3.5130e-02,\n",
      "         -1.8534e-02, -2.6429e-02, -1.3706e-03, -5.0158e-02,  2.3113e-03,\n",
      "          3.9490e-02, -3.2272e-02, -1.4337e-02,  1.4517e-02, -1.0037e-03,\n",
      "         -1.5120e-03,  5.0899e-02, -4.2425e-02, -4.9370e-02, -4.5330e-02,\n",
      "         -5.3768e-02, -5.2764e-02, -4.7216e-03, -7.4714e-03,  5.0246e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0198, -0.0511,  0.0556], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0165, -0.0241,  0.0451,  ..., -0.0109,  0.0137,  0.0035],\n",
      "        [ 0.0328,  0.0502,  0.0171,  ...,  0.0129, -0.0311,  0.0174],\n",
      "        [-0.0090, -0.0423,  0.0061,  ..., -0.0264,  0.0513, -0.0204],\n",
      "        ...,\n",
      "        [ 0.0199,  0.0356,  0.0194,  ..., -0.0515, -0.0254,  0.0471],\n",
      "        [-0.0004,  0.0497, -0.0377,  ...,  0.0204, -0.0040, -0.0026],\n",
      "        [-0.0438, -0.0297, -0.0558,  ...,  0.0030,  0.0523, -0.0395]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0463,  0.0071, -0.0492, -0.0188, -0.0296,  0.0100,  0.0072,  0.0126,\n",
      "        -0.0523,  0.0087, -0.0079], requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeerakw/Documents/PhD/projects/tools/mlearn/mlearn/utils/train.py:205: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), clip)  # Prevent exploding gradients\n",
      "/Users/zeerakw/.virtualenvs/mlearn/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "Batch:   0%|          | 0/200 [00:00<?, ?it/s, batch_loss=0.0104, diff=0.123, epoch_loss=0.0104, f1-score=0.123, task=1]\u001b[A\n",
      "Batch:   0%|          | 1/200 [00:00<00:36,  5.39it/s, batch_loss=0.0104, diff=0.123, epoch_loss=0.0104, f1-score=0.123, task=1]\u001b[A\n",
      "Batch:   0%|          | 1/200 [00:00<00:36,  5.39it/s, batch_loss=0.0204, diff=0.319, epoch_loss=0.0154, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:   1%|          | 2/200 [00:00<00:37,  5.29it/s, batch_loss=0.0204, diff=0.319, epoch_loss=0.0154, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:   1%|          | 2/200 [00:00<00:37,  5.29it/s, batch_loss=0.0104, diff=-.319, epoch_loss=0.0137, f1-score=0.23, task=1] \u001b[A\n",
      "Batch:   2%|▏         | 3/200 [00:00<00:35,  5.62it/s, batch_loss=0.0104, diff=-.319, epoch_loss=0.0137, f1-score=0.23, task=1]\u001b[A\n",
      "Batch:   2%|▏         | 3/200 [00:00<00:35,  5.62it/s, batch_loss=0.0104, diff=0, epoch_loss=0.0129, f1-score=0.203, task=1]   \u001b[A\n",
      "Batch:   2%|▏         | 4/200 [00:00<00:37,  5.29it/s, batch_loss=0.0104, diff=0, epoch_loss=0.0129, f1-score=0.203, task=1]\u001b[A\n",
      "Batch:   2%|▏         | 4/200 [00:00<00:37,  5.29it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0118, f1-score=0.166, task=2]\u001b[A\n",
      "Batch:   2%|▎         | 5/200 [00:00<00:36,  5.38it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0118, f1-score=0.166, task=2]\u001b[A\n",
      "Batch:   2%|▎         | 5/200 [00:01<00:36,  5.38it/s, batch_loss=0.0203, diff=0.426, epoch_loss=0.0132, f1-score=0.212, task=0]\u001b[A\n",
      "Batch:   3%|▎         | 6/200 [00:01<00:34,  5.62it/s, batch_loss=0.0203, diff=0.426, epoch_loss=0.0132, f1-score=0.212, task=0]\u001b[A\n",
      "Batch:   3%|▎         | 6/200 [00:01<00:34,  5.62it/s, batch_loss=0.0204, diff=0, epoch_loss=0.0143, f1-score=0.245, task=0]    \u001b[A\n",
      "Batch:   4%|▎         | 7/200 [00:01<00:34,  5.65it/s, batch_loss=0.0204, diff=0, epoch_loss=0.0143, f1-score=0.245, task=0]\u001b[A\n",
      "Batch:   4%|▎         | 7/200 [00:01<00:34,  5.65it/s, batch_loss=0.0203, diff=0, epoch_loss=0.0150, f1-score=0.27, task=0] \u001b[A\n",
      "Batch:   4%|▍         | 8/200 [00:01<00:32,  5.92it/s, batch_loss=0.0203, diff=0, epoch_loss=0.0150, f1-score=0.27, task=0]\u001b[A\n",
      "Batch:   4%|▍         | 8/200 [00:01<00:32,  5.92it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.241, task=2]\u001b[A\n",
      "Batch:   4%|▍         | 9/200 [00:01<00:31,  6.15it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.241, task=2]\u001b[A\n",
      "Batch:   4%|▍         | 9/200 [00:01<00:31,  6.15it/s, batch_loss=0.0203, diff=0.426, epoch_loss=0.0148, f1-score=0.261, task=0]\u001b[A\n",
      "Batch:   5%|▌         | 10/200 [00:01<00:29,  6.55it/s, batch_loss=0.0203, diff=0.426, epoch_loss=0.0148, f1-score=0.261, task=0]\u001b[A\n",
      "Batch:   5%|▌         | 10/200 [00:01<00:29,  6.55it/s, batch_loss=0.0202, diff=0, epoch_loss=0.0153, f1-score=0.278, task=0]    \u001b[A\n",
      "Batch:   6%|▌         | 11/200 [00:01<00:27,  6.75it/s, batch_loss=0.0202, diff=0, epoch_loss=0.0153, f1-score=0.278, task=0]\u001b[A\n",
      "Batch:   6%|▌         | 11/200 [00:01<00:27,  6.75it/s, batch_loss=0.0104, diff=-.319, epoch_loss=0.0149, f1-score=0.265, task=1]\u001b[A\n",
      "Batch:   6%|▌         | 12/200 [00:01<00:28,  6.68it/s, batch_loss=0.0104, diff=-.319, epoch_loss=0.0149, f1-score=0.265, task=1]\u001b[A\n",
      "Batch:   6%|▌         | 12/200 [00:02<00:28,  6.68it/s, batch_loss=0.0202, diff=0.319, epoch_loss=0.0153, f1-score=0.279, task=0]\u001b[A\n",
      "Batch:   6%|▋         | 13/200 [00:02<00:28,  6.63it/s, batch_loss=0.0202, diff=0.319, epoch_loss=0.0153, f1-score=0.279, task=0]\u001b[A\n",
      "Batch:   6%|▋         | 13/200 [00:02<00:28,  6.63it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0149, f1-score=0.268, task=1]\u001b[A\n",
      "Batch:   7%|▋         | 14/200 [00:02<00:27,  6.72it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0149, f1-score=0.268, task=1]\u001b[A\n",
      "Batch:   7%|▋         | 14/200 [00:02<00:27,  6.72it/s, batch_loss=0.0201, diff=0.319, epoch_loss=0.0153, f1-score=0.279, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 15/200 [00:02<00:26,  6.93it/s, batch_loss=0.0201, diff=0.319, epoch_loss=0.0153, f1-score=0.279, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 15/200 [00:02<00:26,  6.93it/s, batch_loss=0.0201, diff=0, epoch_loss=0.0156, f1-score=0.289, task=0]    \u001b[A\n",
      "Batch:   8%|▊         | 16/200 [00:02<00:26,  6.96it/s, batch_loss=0.0201, diff=0, epoch_loss=0.0156, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 16/200 [00:02<00:26,  6.96it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.273, task=2]\u001b[A\n",
      "Batch:   8%|▊         | 17/200 [00:02<00:27,  6.74it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.273, task=2]\u001b[A\n",
      "Batch:   8%|▊         | 17/200 [00:02<00:27,  6.74it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0147, f1-score=0.259, task=2]    \u001b[A\n",
      "Batch:   9%|▉         | 18/200 [00:02<00:27,  6.57it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0147, f1-score=0.259, task=2]\u001b[A\n",
      "Batch:   9%|▉         | 18/200 [00:03<00:27,  6.57it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0143, f1-score=0.246, task=2]\u001b[A\n",
      "Batch:  10%|▉         | 19/200 [00:03<00:26,  6.83it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0143, f1-score=0.246, task=2]\u001b[A\n",
      "Batch:  10%|▉         | 19/200 [00:03<00:26,  6.83it/s, batch_loss=0.0103, diff=0.107, epoch_loss=0.0141, f1-score=0.24, task=1]\u001b[A\n",
      "Batch:  10%|█         | 20/200 [00:03<00:26,  6.76it/s, batch_loss=0.0103, diff=0.107, epoch_loss=0.0141, f1-score=0.24, task=1]\u001b[A\n",
      "Batch:  10%|█         | 20/200 [00:03<00:26,  6.76it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0138, f1-score=0.229, task=2]\u001b[A\n",
      "Batch:  10%|█         | 21/200 [00:03<00:25,  6.89it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0138, f1-score=0.229, task=2]\u001b[A\n",
      "Batch:  10%|█         | 21/200 [00:03<00:25,  6.89it/s, batch_loss=0.0201, diff=0.426, epoch_loss=0.0141, f1-score=0.239, task=0]\u001b[A\n",
      "Batch:  11%|█         | 22/200 [00:03<00:26,  6.73it/s, batch_loss=0.0201, diff=0.426, epoch_loss=0.0141, f1-score=0.239, task=0]\u001b[A\n",
      "Batch:  11%|█         | 22/200 [00:03<00:26,  6.73it/s, batch_loss=0.0200, diff=0, epoch_loss=0.0143, f1-score=0.248, task=0]    \u001b[A\n",
      "Batch:  12%|█▏        | 23/200 [00:03<00:27,  6.40it/s, batch_loss=0.0200, diff=0, epoch_loss=0.0143, f1-score=0.248, task=0]\u001b[A\n",
      "Batch:  12%|█▏        | 23/200 [00:03<00:27,  6.40it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0146, f1-score=0.256, task=0]\u001b[A\n",
      "Batch:  12%|█▏        | 24/200 [00:03<00:27,  6.49it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0146, f1-score=0.256, task=0]\u001b[A\n",
      "Batch:  12%|█▏        | 24/200 [00:03<00:27,  6.49it/s, batch_loss=0.0200, diff=0, epoch_loss=0.0148, f1-score=0.264, task=0]\u001b[A\n",
      "Batch:  12%|█▎        | 25/200 [00:03<00:26,  6.57it/s, batch_loss=0.0200, diff=0, epoch_loss=0.0148, f1-score=0.264, task=0]\u001b[A\n",
      "Batch:  12%|█▎        | 25/200 [00:04<00:26,  6.57it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0150, f1-score=0.27, task=0] \u001b[A\n",
      "Batch:  13%|█▎        | 26/200 [00:04<00:25,  6.74it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0150, f1-score=0.27, task=0]\u001b[A\n",
      "Batch:  13%|█▎        | 26/200 [00:04<00:25,  6.74it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0152, f1-score=0.277, task=0]\u001b[A\n",
      "Batch:  14%|█▎        | 27/200 [00:04<00:27,  6.21it/s, batch_loss=0.0199, diff=0, epoch_loss=0.0152, f1-score=0.277, task=0]\u001b[A\n",
      "Batch:  14%|█▎        | 27/200 [00:04<00:27,  6.21it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0153, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 28/200 [00:04<00:27,  6.22it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0153, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 28/200 [00:04<00:27,  6.22it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0155, f1-score=0.288, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 29/200 [00:04<00:26,  6.34it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0155, f1-score=0.288, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  14%|█▍        | 29/200 [00:04<00:26,  6.34it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0156, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  15%|█▌        | 30/200 [00:04<00:26,  6.45it/s, batch_loss=0.0198, diff=0, epoch_loss=0.0156, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  15%|█▌        | 30/200 [00:04<00:26,  6.45it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0155, f1-score=0.288, task=1]\u001b[A\n",
      "Batch:  16%|█▌        | 31/200 [00:04<00:25,  6.56it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0155, f1-score=0.288, task=1]\u001b[A\n",
      "Batch:  16%|█▌        | 31/200 [00:04<00:25,  6.56it/s, batch_loss=0.0198, diff=0.319, epoch_loss=0.0156, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 32/200 [00:04<00:23,  7.05it/s, batch_loss=0.0198, diff=0.319, epoch_loss=0.0156, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 32/200 [00:05<00:23,  7.05it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.284, task=2]\u001b[A\n",
      "Batch:  16%|█▋        | 33/200 [00:05<00:23,  7.17it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.284, task=2]\u001b[A\n",
      "Batch:  16%|█▋        | 33/200 [00:05<00:23,  7.17it/s, batch_loss=0.0197, diff=0.426, epoch_loss=0.0155, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  17%|█▋        | 34/200 [00:05<00:23,  7.14it/s, batch_loss=0.0197, diff=0.426, epoch_loss=0.0155, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  17%|█▋        | 34/200 [00:05<00:23,  7.14it/s, batch_loss=0.0197, diff=0, epoch_loss=0.0156, f1-score=0.293, task=0]    \u001b[A\n",
      "Batch:  18%|█▊        | 35/200 [00:05<00:23,  7.13it/s, batch_loss=0.0197, diff=0, epoch_loss=0.0156, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  18%|█▊        | 35/200 [00:05<00:23,  7.13it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0155, f1-score=0.289, task=1]\u001b[A\n",
      "Batch:  18%|█▊        | 36/200 [00:05<00:22,  7.29it/s, batch_loss=0.0103, diff=-.319, epoch_loss=0.0155, f1-score=0.289, task=1]\u001b[A\n",
      "Batch:  18%|█▊        | 36/200 [00:05<00:22,  7.29it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0152, f1-score=0.281, task=2]\u001b[A\n",
      "Batch:  18%|█▊        | 37/200 [00:05<00:21,  7.48it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0152, f1-score=0.281, task=2]\u001b[A\n",
      "Batch:  18%|█▊        | 37/200 [00:05<00:21,  7.48it/s, batch_loss=0.0102, diff=0.107, epoch_loss=0.0151, f1-score=0.277, task=1]\u001b[A\n",
      "Batch:  19%|█▉        | 38/200 [00:05<00:21,  7.44it/s, batch_loss=0.0102, diff=0.107, epoch_loss=0.0151, f1-score=0.277, task=1]\u001b[A\n",
      "Batch:  19%|█▉        | 38/200 [00:05<00:21,  7.44it/s, batch_loss=0.0197, diff=0.319, epoch_loss=0.0152, f1-score=0.281, task=0]\u001b[A\n",
      "Batch:  20%|█▉        | 39/200 [00:05<00:20,  7.73it/s, batch_loss=0.0197, diff=0.319, epoch_loss=0.0152, f1-score=0.281, task=0]\u001b[A\n",
      "Batch:  20%|█▉        | 39/200 [00:06<00:20,  7.73it/s, batch_loss=0.0196, diff=0, epoch_loss=0.0153, f1-score=0.285, task=0]    \u001b[A\n",
      "Batch:  20%|██        | 40/200 [00:06<00:21,  7.59it/s, batch_loss=0.0196, diff=0, epoch_loss=0.0153, f1-score=0.285, task=0]\u001b[A\n",
      "Batch:  20%|██        | 40/200 [00:06<00:21,  7.59it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.279, task=2]\u001b[A\n",
      "Batch:  20%|██        | 41/200 [00:06<00:21,  7.50it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.279, task=2]\u001b[A\n",
      "Batch:  20%|██        | 41/200 [00:06<00:21,  7.50it/s, batch_loss=0.0196, diff=0.426, epoch_loss=0.0153, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:  21%|██        | 42/200 [00:06<00:20,  7.65it/s, batch_loss=0.0196, diff=0.426, epoch_loss=0.0153, f1-score=0.283, task=0]\u001b[A\n",
      "Batch:  21%|██        | 42/200 [00:06<00:20,  7.65it/s, batch_loss=0.0196, diff=0, epoch_loss=0.0154, f1-score=0.286, task=0]    \u001b[A\n",
      "Batch:  22%|██▏       | 43/200 [00:06<00:21,  7.46it/s, batch_loss=0.0196, diff=0, epoch_loss=0.0154, f1-score=0.286, task=0]\u001b[A\n",
      "Batch:  22%|██▏       | 43/200 [00:06<00:21,  7.46it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0152, f1-score=0.283, task=1]\u001b[A\n",
      "Batch:  22%|██▏       | 44/200 [00:06<00:20,  7.51it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0152, f1-score=0.283, task=1]\u001b[A\n",
      "Batch:  22%|██▏       | 44/200 [00:06<00:20,  7.51it/s, batch_loss=0.0196, diff=0.319, epoch_loss=0.0153, f1-score=0.286, task=0]\u001b[A\n",
      "Batch:  22%|██▎       | 45/200 [00:06<00:20,  7.70it/s, batch_loss=0.0196, diff=0.319, epoch_loss=0.0153, f1-score=0.286, task=0]\u001b[A\n",
      "Batch:  22%|██▎       | 45/200 [00:07<00:20,  7.70it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0154, f1-score=0.29, task=0]     \u001b[A\n",
      "Batch:  23%|██▎       | 46/200 [00:07<00:31,  4.91it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0154, f1-score=0.29, task=0]\u001b[A\n",
      "Batch:  23%|██▎       | 46/200 [00:07<00:31,  4.91it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0155, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  24%|██▎       | 47/200 [00:07<00:41,  3.73it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0155, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  24%|██▎       | 47/200 [00:07<00:41,  3.73it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0156, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  24%|██▍       | 48/200 [00:07<00:38,  3.93it/s, batch_loss=0.0195, diff=0, epoch_loss=0.0156, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  24%|██▍       | 48/200 [00:07<00:38,  3.93it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.29, task=2]\u001b[A\n",
      "Batch:  24%|██▍       | 49/200 [00:07<00:33,  4.57it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.29, task=2]\u001b[A\n",
      "Batch:  24%|██▍       | 49/200 [00:08<00:33,  4.57it/s, batch_loss=0.0102, diff=0.107, epoch_loss=0.0153, f1-score=0.287, task=1]\u001b[A\n",
      "Batch:  25%|██▌       | 50/200 [00:08<00:33,  4.54it/s, batch_loss=0.0102, diff=0.107, epoch_loss=0.0153, f1-score=0.287, task=1]\u001b[A\n",
      "Batch:  25%|██▌       | 50/200 [00:08<00:33,  4.54it/s, batch_loss=0.0194, diff=0.319, epoch_loss=0.0154, f1-score=0.29, task=0] \u001b[A\n",
      "Batch:  26%|██▌       | 51/200 [00:08<00:44,  3.37it/s, batch_loss=0.0194, diff=0.319, epoch_loss=0.0154, f1-score=0.29, task=0]\u001b[A\n",
      "Batch:  26%|██▌       | 51/200 [00:08<00:44,  3.37it/s, batch_loss=0.0194, diff=0, epoch_loss=0.0155, f1-score=0.293, task=0]   \u001b[A\n",
      "Batch:  26%|██▌       | 52/200 [00:08<00:42,  3.45it/s, batch_loss=0.0194, diff=0, epoch_loss=0.0155, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  26%|██▌       | 52/200 [00:09<00:42,  3.45it/s, batch_loss=0.0194, diff=0, epoch_loss=0.0156, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  26%|██▋       | 53/200 [00:09<00:38,  3.79it/s, batch_loss=0.0194, diff=0, epoch_loss=0.0156, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  26%|██▋       | 53/200 [00:09<00:38,  3.79it/s, batch_loss=0.0193, diff=0, epoch_loss=0.0156, f1-score=0.298, task=0]\u001b[A\n",
      "Batch:  27%|██▋       | 54/200 [00:09<00:38,  3.83it/s, batch_loss=0.0193, diff=0, epoch_loss=0.0156, f1-score=0.298, task=0]\u001b[A\n",
      "Batch:  27%|██▋       | 54/200 [00:09<00:38,  3.83it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  28%|██▊       | 55/200 [00:09<00:34,  4.23it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  28%|██▊       | 55/200 [00:09<00:34,  4.23it/s, batch_loss=0.0193, diff=0.426, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 56/200 [00:09<00:31,  4.64it/s, batch_loss=0.0193, diff=0.426, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 56/200 [00:09<00:31,  4.64it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0155, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  28%|██▊       | 57/200 [00:09<00:34,  4.16it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0155, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  28%|██▊       | 57/200 [00:10<00:34,  4.16it/s, batch_loss=0.0193, diff=0.319, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  29%|██▉       | 58/200 [00:10<00:32,  4.39it/s, batch_loss=0.0193, diff=0.319, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  29%|██▉       | 58/200 [00:10<00:32,  4.39it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.291, task=2]\u001b[A\n",
      "Batch:  30%|██▉       | 59/200 [00:10<00:31,  4.45it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.291, task=2]\u001b[A\n",
      "Batch:  30%|██▉       | 59/200 [00:10<00:31,  4.45it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0153, f1-score=0.286, task=2]    \u001b[A\n",
      "Batch:  30%|███       | 60/200 [00:10<00:30,  4.60it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0153, f1-score=0.286, task=2]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  30%|███       | 60/200 [00:10<00:30,  4.60it/s, batch_loss=0.0193, diff=0.426, epoch_loss=0.0153, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  30%|███       | 61/200 [00:10<00:30,  4.58it/s, batch_loss=0.0193, diff=0.426, epoch_loss=0.0153, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  30%|███       | 61/200 [00:11<00:30,  4.58it/s, batch_loss=0.0193, diff=0, epoch_loss=0.0154, f1-score=0.291, task=0]    \u001b[A\n",
      "Batch:  31%|███       | 62/200 [00:11<00:32,  4.25it/s, batch_loss=0.0193, diff=0, epoch_loss=0.0154, f1-score=0.291, task=0]\u001b[A\n",
      "Batch:  31%|███       | 62/200 [00:11<00:32,  4.25it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0153, f1-score=0.287, task=2]\u001b[A\n",
      "Batch:  32%|███▏      | 63/200 [00:11<00:29,  4.66it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0153, f1-score=0.287, task=2]\u001b[A\n",
      "Batch:  32%|███▏      | 63/200 [00:11<00:29,  4.66it/s, batch_loss=0.0192, diff=0.426, epoch_loss=0.0153, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  32%|███▏      | 64/200 [00:11<00:28,  4.83it/s, batch_loss=0.0192, diff=0.426, epoch_loss=0.0153, f1-score=0.289, task=0]\u001b[A\n",
      "Batch:  32%|███▏      | 64/200 [00:11<00:28,  4.83it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0154, f1-score=0.292, task=0]    \u001b[A\n",
      "Batch:  32%|███▎      | 65/200 [00:11<00:28,  4.81it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0154, f1-score=0.292, task=0]\u001b[A\n",
      "Batch:  32%|███▎      | 65/200 [00:11<00:28,  4.81it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0154, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  33%|███▎      | 66/200 [00:11<00:26,  5.07it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0154, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  33%|███▎      | 66/200 [00:11<00:26,  5.07it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  34%|███▎      | 67/200 [00:11<00:26,  4.93it/s, batch_loss=0.0192, diff=0, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  34%|███▎      | 67/200 [00:12<00:26,  4.93it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.294, task=1]\u001b[A\n",
      "Batch:  34%|███▍      | 68/200 [00:12<00:25,  5.15it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.294, task=1]\u001b[A\n",
      "Batch:  34%|███▍      | 68/200 [00:12<00:25,  5.15it/s, batch_loss=0.0191, diff=0.319, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  34%|███▍      | 69/200 [00:12<00:24,  5.45it/s, batch_loss=0.0191, diff=0.319, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  34%|███▍      | 69/200 [00:12<00:24,  5.45it/s, batch_loss=0.0191, diff=0, epoch_loss=0.0155, f1-score=0.298, task=0]    \u001b[A\n",
      "Batch:  35%|███▌      | 70/200 [00:12<00:26,  4.97it/s, batch_loss=0.0191, diff=0, epoch_loss=0.0155, f1-score=0.298, task=0]\u001b[A\n",
      "Batch:  35%|███▌      | 70/200 [00:12<00:26,  4.97it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.294, task=2]\u001b[A\n",
      "Batch:  36%|███▌      | 71/200 [00:12<00:24,  5.19it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0154, f1-score=0.294, task=2]\u001b[A\n",
      "Batch:  36%|███▌      | 71/200 [00:12<00:24,  5.19it/s, batch_loss=0.0191, diff=0.426, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 72/200 [00:12<00:24,  5.29it/s, batch_loss=0.0191, diff=0.426, epoch_loss=0.0155, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 72/200 [00:13<00:24,  5.29it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.294, task=1]\u001b[A\n",
      "Batch:  36%|███▋      | 73/200 [00:13<00:27,  4.69it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.294, task=1]\u001b[A\n",
      "Batch:  36%|███▋      | 73/200 [00:13<00:27,  4.69it/s, batch_loss=0.0191, diff=0.319, epoch_loss=0.0154, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  37%|███▋      | 74/200 [00:13<00:26,  4.81it/s, batch_loss=0.0191, diff=0.319, epoch_loss=0.0154, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  37%|███▋      | 74/200 [00:13<00:26,  4.81it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  38%|███▊      | 75/200 [00:13<00:26,  4.79it/s, batch_loss=0.0102, diff=-.319, epoch_loss=0.0154, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  38%|███▊      | 75/200 [00:13<00:26,  4.79it/s, batch_loss=0.0190, diff=0.319, epoch_loss=0.0154, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 76/200 [00:13<00:30,  4.13it/s, batch_loss=0.0190, diff=0.319, epoch_loss=0.0154, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 76/200 [00:14<00:30,  4.13it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0155, f1-score=0.297, task=0]    \u001b[A\n",
      "Batch:  38%|███▊      | 77/200 [00:14<00:27,  4.48it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0155, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 77/200 [00:14<00:27,  4.48it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0155, f1-score=0.299, task=0]\u001b[A\n",
      "Batch:  39%|███▉      | 78/200 [00:14<00:28,  4.34it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0155, f1-score=0.299, task=0]\u001b[A\n",
      "Batch:  39%|███▉      | 78/200 [00:14<00:28,  4.34it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  40%|███▉      | 79/200 [00:14<00:25,  4.67it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  40%|███▉      | 79/200 [00:14<00:25,  4.67it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  40%|████      | 80/200 [00:14<00:25,  4.66it/s, batch_loss=0.0190, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  40%|████      | 80/200 [00:14<00:25,  4.66it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.299, task=2]\u001b[A\n",
      "Batch:  40%|████      | 81/200 [00:14<00:23,  5.06it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.299, task=2]\u001b[A\n",
      "Batch:  40%|████      | 81/200 [00:15<00:23,  5.06it/s, batch_loss=0.0189, diff=0.426, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  41%|████      | 82/200 [00:15<00:22,  5.24it/s, batch_loss=0.0189, diff=0.426, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  41%|████      | 82/200 [00:15<00:22,  5.24it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]    \u001b[A\n",
      "Batch:  42%|████▏     | 83/200 [00:15<00:20,  5.59it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  42%|████▏     | 83/200 [00:15<00:20,  5.59it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.299, task=2]\u001b[A\n",
      "Batch:  42%|████▏     | 84/200 [00:15<00:21,  5.31it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0155, f1-score=0.299, task=2]\u001b[A\n",
      "Batch:  42%|████▏     | 84/200 [00:15<00:21,  5.31it/s, batch_loss=0.0189, diff=0.426, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  42%|████▎     | 85/200 [00:15<00:22,  5.14it/s, batch_loss=0.0189, diff=0.426, epoch_loss=0.0155, f1-score=0.301, task=0]\u001b[A\n",
      "Batch:  42%|████▎     | 85/200 [00:15<00:22,  5.14it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]    \u001b[A\n",
      "Batch:  43%|████▎     | 86/200 [00:15<00:23,  4.76it/s, batch_loss=0.0189, diff=0, epoch_loss=0.0156, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  43%|████▎     | 86/200 [00:16<00:23,  4.76it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0156, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  44%|████▎     | 87/200 [00:16<00:24,  4.70it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0156, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  44%|████▎     | 87/200 [00:16<00:24,  4.70it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0156, f1-score=0.306, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 88/200 [00:16<00:22,  4.91it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0156, f1-score=0.306, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 88/200 [00:16<00:22,  4.91it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0157, f1-score=0.307, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 89/200 [00:16<00:22,  4.94it/s, batch_loss=0.0188, diff=0, epoch_loss=0.0157, f1-score=0.307, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 89/200 [00:16<00:22,  4.94it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0156, f1-score=0.305, task=1]\u001b[A\n",
      "Batch:  45%|████▌     | 90/200 [00:16<00:20,  5.37it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0156, f1-score=0.305, task=1]\u001b[A\n",
      "Batch:  45%|████▌     | 90/200 [00:16<00:20,  5.37it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0155, f1-score=0.302, task=2]\u001b[A\n",
      "Batch:  46%|████▌     | 91/200 [00:16<00:18,  5.83it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0155, f1-score=0.302, task=2]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  46%|████▌     | 91/200 [00:16<00:18,  5.83it/s, batch_loss=0.0188, diff=0.426, epoch_loss=0.0156, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  46%|████▌     | 92/200 [00:16<00:20,  5.37it/s, batch_loss=0.0188, diff=0.426, epoch_loss=0.0156, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  46%|████▌     | 92/200 [00:17<00:20,  5.37it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0155, f1-score=0.302, task=1]\u001b[A\n",
      "Batch:  46%|████▋     | 93/200 [00:17<00:20,  5.26it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0155, f1-score=0.302, task=1]\u001b[A\n",
      "Batch:  46%|████▋     | 93/200 [00:17<00:20,  5.26it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0155, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  47%|████▋     | 94/200 [00:17<00:18,  5.72it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0155, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  47%|████▋     | 94/200 [00:17<00:18,  5.72it/s, batch_loss=0.0187, diff=0, epoch_loss=0.0156, f1-score=0.305, task=0]    \u001b[A\n",
      "Batch:  48%|████▊     | 95/200 [00:17<00:21,  4.99it/s, batch_loss=0.0187, diff=0, epoch_loss=0.0156, f1-score=0.305, task=0]\u001b[A\n",
      "Batch:  48%|████▊     | 95/200 [00:17<00:21,  4.99it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0155, f1-score=0.303, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 96/200 [00:17<00:19,  5.24it/s, batch_loss=0.0101, diff=-.319, epoch_loss=0.0155, f1-score=0.303, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 96/200 [00:17<00:19,  5.24it/s, batch_loss=0.0101, diff=0, epoch_loss=0.0154, f1-score=0.301, task=1]    \u001b[A\n",
      "Batch:  48%|████▊     | 97/200 [00:17<00:20,  5.13it/s, batch_loss=0.0101, diff=0, epoch_loss=0.0154, f1-score=0.301, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 97/200 [00:18<00:20,  5.13it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0154, f1-score=0.298, task=2]\u001b[A\n",
      "Batch:  49%|████▉     | 98/200 [00:18<00:21,  4.79it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0154, f1-score=0.298, task=2]\u001b[A\n",
      "Batch:  49%|████▉     | 98/200 [00:18<00:21,  4.79it/s, batch_loss=0.0101, diff=0.107, epoch_loss=0.0153, f1-score=0.296, task=1]\u001b[A\n",
      "Batch:  50%|████▉     | 99/200 [00:18<00:19,  5.24it/s, batch_loss=0.0101, diff=0.107, epoch_loss=0.0153, f1-score=0.296, task=1]\u001b[A\n",
      "Batch:  50%|████▉     | 99/200 [00:18<00:19,  5.24it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0153, f1-score=0.298, task=0]\u001b[A\n",
      "Batch:  50%|█████     | 100/200 [00:18<00:18,  5.40it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0153, f1-score=0.298, task=0]\u001b[A\n",
      "Batch:  50%|█████     | 100/200 [00:18<00:18,  5.40it/s, batch_loss=0.0187, diff=0, epoch_loss=0.0154, f1-score=0.299, task=0]    \u001b[A\n",
      "Batch:  50%|█████     | 101/200 [00:18<00:18,  5.32it/s, batch_loss=0.0187, diff=0, epoch_loss=0.0154, f1-score=0.299, task=0]\u001b[A\n",
      "Batch:  50%|█████     | 101/200 [00:18<00:18,  5.32it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0153, f1-score=0.297, task=1]\u001b[A\n",
      "Batch:  51%|█████     | 102/200 [00:18<00:17,  5.65it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0153, f1-score=0.297, task=1]\u001b[A\n",
      "Batch:  51%|█████     | 102/200 [00:19<00:17,  5.65it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0153, f1-score=0.295, task=2]\u001b[A\n",
      "Batch:  52%|█████▏    | 103/200 [00:19<00:17,  5.62it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0153, f1-score=0.295, task=2]\u001b[A\n",
      "Batch:  52%|█████▏    | 103/200 [00:19<00:17,  5.62it/s, batch_loss=0.0100, diff=0.107, epoch_loss=0.0152, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  52%|█████▏    | 104/200 [00:19<00:16,  5.70it/s, batch_loss=0.0100, diff=0.107, epoch_loss=0.0152, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  52%|█████▏    | 104/200 [00:19<00:16,  5.70it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  52%|█████▎    | 105/200 [00:19<00:16,  5.93it/s, batch_loss=0.0187, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  52%|█████▎    | 105/200 [00:19<00:16,  5.93it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.296, task=0]    \u001b[A\n",
      "Batch:  53%|█████▎    | 106/200 [00:19<00:16,  5.65it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  53%|█████▎    | 106/200 [00:19<00:16,  5.65it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  54%|█████▎    | 107/200 [00:19<00:16,  5.67it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  54%|█████▎    | 107/200 [00:19<00:16,  5.67it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0152, f1-score=0.295, task=2]\u001b[A\n",
      "Batch:  54%|█████▍    | 108/200 [00:19<00:16,  5.63it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0152, f1-score=0.295, task=2]\u001b[A\n",
      "Batch:  54%|█████▍    | 108/200 [00:20<00:16,  5.63it/s, batch_loss=0.0100, diff=0.107, epoch_loss=0.0152, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  55%|█████▍    | 109/200 [00:20<00:18,  5.04it/s, batch_loss=0.0100, diff=0.107, epoch_loss=0.0152, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  55%|█████▍    | 109/200 [00:20<00:18,  5.04it/s, batch_loss=0.0186, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  55%|█████▌    | 110/200 [00:20<00:19,  4.68it/s, batch_loss=0.0186, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  55%|█████▌    | 110/200 [00:20<00:19,  4.68it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0152, f1-score=0.296, task=0]    \u001b[A\n",
      "Batch:  56%|█████▌    | 111/200 [00:20<00:18,  4.83it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0152, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 111/200 [00:20<00:18,  4.83it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 112/200 [00:20<00:19,  4.56it/s, batch_loss=0.0186, diff=0, epoch_loss=0.0153, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 112/200 [00:21<00:19,  4.56it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0152, f1-score=0.295, task=1]\u001b[A\n",
      "Batch:  56%|█████▋    | 113/200 [00:21<00:18,  4.78it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0152, f1-score=0.295, task=1]\u001b[A\n",
      "Batch:  56%|█████▋    | 113/200 [00:21<00:18,  4.78it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0152, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  57%|█████▋    | 114/200 [00:21<00:18,  4.68it/s, batch_loss=0.0075, diff=-.107, epoch_loss=0.0152, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  57%|█████▋    | 114/200 [00:21<00:18,  4.68it/s, batch_loss=0.0185, diff=0.426, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  57%|█████▊    | 115/200 [00:21<00:17,  4.73it/s, batch_loss=0.0185, diff=0.426, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  57%|█████▊    | 115/200 [00:21<00:17,  4.73it/s, batch_loss=0.0100, diff=-.27, epoch_loss=0.0151, f1-score=0.293, task=1] \u001b[A\n",
      "Batch:  58%|█████▊    | 116/200 [00:21<00:17,  4.90it/s, batch_loss=0.0100, diff=-.27, epoch_loss=0.0151, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  58%|█████▊    | 116/200 [00:21<00:17,  4.90it/s, batch_loss=0.0185, diff=0.27, epoch_loss=0.0152, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  58%|█████▊    | 117/200 [00:21<00:17,  4.83it/s, batch_loss=0.0185, diff=0.27, epoch_loss=0.0152, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  58%|█████▊    | 117/200 [00:22<00:17,  4.83it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0151, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  59%|█████▉    | 118/200 [00:22<00:17,  4.70it/s, batch_loss=0.0100, diff=-.319, epoch_loss=0.0151, f1-score=0.293, task=1]\u001b[A\n",
      "Batch:  59%|█████▉    | 118/200 [00:22<00:17,  4.70it/s, batch_loss=0.0185, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  60%|█████▉    | 119/200 [00:22<00:15,  5.28it/s, batch_loss=0.0185, diff=0.319, epoch_loss=0.0152, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  60%|█████▉    | 119/200 [00:22<00:15,  5.28it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  60%|██████    | 120/200 [00:22<00:16,  4.90it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0151, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  60%|██████    | 120/200 [00:22<00:16,  4.90it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0150, f1-score=0.29, task=2]     \u001b[A\n",
      "Batch:  60%|██████    | 121/200 [00:22<00:15,  5.08it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0150, f1-score=0.29, task=2]\u001b[A\n",
      "Batch:  60%|██████    | 121/200 [00:22<00:15,  5.08it/s, batch_loss=0.0185, diff=0.426, epoch_loss=0.0151, f1-score=0.291, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  61%|██████    | 122/200 [00:22<00:15,  4.89it/s, batch_loss=0.0185, diff=0.426, epoch_loss=0.0151, f1-score=0.291, task=0]\u001b[A\n",
      "Batch:  61%|██████    | 122/200 [00:23<00:15,  4.89it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.292, task=0]    \u001b[A\n",
      "Batch:  62%|██████▏   | 123/200 [00:23<00:17,  4.43it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.292, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 123/200 [00:23<00:17,  4.43it/s, batch_loss=0.0185, diff=0, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 124/200 [00:23<00:15,  4.95it/s, batch_loss=0.0185, diff=0, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 124/200 [00:23<00:15,  4.95it/s, batch_loss=0.0099, diff=-.33, epoch_loss=0.0151, f1-score=0.292, task=1]\u001b[A\n",
      "Batch:  62%|██████▎   | 125/200 [00:23<00:15,  4.96it/s, batch_loss=0.0099, diff=-.33, epoch_loss=0.0151, f1-score=0.292, task=1]\u001b[A\n",
      "Batch:  62%|██████▎   | 125/200 [00:23<00:15,  4.96it/s, batch_loss=0.0099, diff=0.132, epoch_loss=0.0150, f1-score=0.292, task=1]\u001b[A\n",
      "Batch:  63%|██████▎   | 126/200 [00:23<00:15,  4.91it/s, batch_loss=0.0099, diff=0.132, epoch_loss=0.0150, f1-score=0.292, task=1]\u001b[A\n",
      "Batch:  63%|██████▎   | 126/200 [00:23<00:15,  4.91it/s, batch_loss=0.0185, diff=0.198, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  64%|██████▎   | 127/200 [00:23<00:14,  5.17it/s, batch_loss=0.0185, diff=0.198, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  64%|██████▎   | 127/200 [00:24<00:14,  5.17it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.294, task=0]    \u001b[A\n",
      "Batch:  64%|██████▍   | 128/200 [00:24<00:14,  5.08it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  64%|██████▍   | 128/200 [00:24<00:14,  5.08it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  64%|██████▍   | 129/200 [00:24<00:14,  5.01it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  64%|██████▍   | 129/200 [00:24<00:14,  5.01it/s, batch_loss=0.0184, diff=0.426, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  65%|██████▌   | 130/200 [00:24<00:13,  5.28it/s, batch_loss=0.0184, diff=0.426, epoch_loss=0.0151, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  65%|██████▌   | 130/200 [00:24<00:13,  5.28it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.294, task=0]    \u001b[A\n",
      "Batch:  66%|██████▌   | 131/200 [00:24<00:14,  4.75it/s, batch_loss=0.0184, diff=0, epoch_loss=0.0151, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  66%|██████▌   | 131/200 [00:24<00:14,  4.75it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  66%|██████▌   | 132/200 [00:24<00:13,  4.94it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.292, task=2]\u001b[A\n",
      "Batch:  66%|██████▌   | 132/200 [00:25<00:13,  4.94it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0150, f1-score=0.29, task=2]     \u001b[A\n",
      "Batch:  66%|██████▋   | 133/200 [00:25<00:13,  4.86it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0150, f1-score=0.29, task=2]\u001b[A\n",
      "Batch:  66%|██████▋   | 133/200 [00:25<00:13,  4.86it/s, batch_loss=0.0099, diff=0.211, epoch_loss=0.0149, f1-score=0.289, task=1]\u001b[A\n",
      "Batch:  67%|██████▋   | 134/200 [00:25<00:14,  4.51it/s, batch_loss=0.0099, diff=0.211, epoch_loss=0.0149, f1-score=0.289, task=1]\u001b[A\n",
      "Batch:  67%|██████▋   | 134/200 [00:25<00:14,  4.51it/s, batch_loss=0.0183, diff=0.215, epoch_loss=0.0149, f1-score=0.291, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 135/200 [00:25<00:13,  4.97it/s, batch_loss=0.0183, diff=0.215, epoch_loss=0.0149, f1-score=0.291, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 135/200 [00:25<00:13,  4.97it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.292, task=0]    \u001b[A\n",
      "Batch:  68%|██████▊   | 136/200 [00:25<00:13,  4.86it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.292, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 136/200 [00:25<00:13,  4.86it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 137/200 [00:25<00:12,  4.93it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.293, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 137/200 [00:26<00:12,  4.93it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  69%|██████▉   | 138/200 [00:26<00:12,  4.88it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  69%|██████▉   | 138/200 [00:26<00:12,  4.88it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  70%|██████▉   | 139/200 [00:26<00:12,  4.91it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.295, task=0]\u001b[A\n",
      "Batch:  70%|██████▉   | 139/200 [00:26<00:12,  4.91it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  70%|███████   | 140/200 [00:26<00:12,  4.77it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0150, f1-score=0.293, task=2]\u001b[A\n",
      "Batch:  70%|███████   | 140/200 [00:26<00:12,  4.77it/s, batch_loss=0.0183, diff=0.426, epoch_loss=0.0150, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  70%|███████   | 141/200 [00:26<00:11,  5.19it/s, batch_loss=0.0183, diff=0.426, epoch_loss=0.0150, f1-score=0.294, task=0]\u001b[A\n",
      "Batch:  70%|███████   | 141/200 [00:26<00:11,  5.19it/s, batch_loss=0.0099, diff=-.0478, epoch_loss=0.0150, f1-score=0.295, task=1]\u001b[A\n",
      "Batch:  71%|███████   | 142/200 [00:26<00:11,  5.26it/s, batch_loss=0.0099, diff=-.0478, epoch_loss=0.0150, f1-score=0.295, task=1]\u001b[A\n",
      "Batch:  71%|███████   | 142/200 [00:27<00:11,  5.26it/s, batch_loss=0.0183, diff=0.0478, epoch_loss=0.0150, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 143/200 [00:27<00:10,  5.47it/s, batch_loss=0.0183, diff=0.0478, epoch_loss=0.0150, f1-score=0.296, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 143/200 [00:27<00:10,  5.47it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.297, task=0]     \u001b[A\n",
      "Batch:  72%|███████▏  | 144/200 [00:27<00:10,  5.46it/s, batch_loss=0.0183, diff=0, epoch_loss=0.0150, f1-score=0.297, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 144/200 [00:27<00:10,  5.46it/s, batch_loss=0.0099, diff=0.00551, epoch_loss=0.0150, f1-score=0.298, task=1]\u001b[A\n",
      "Batch:  72%|███████▎  | 145/200 [00:27<00:10,  5.45it/s, batch_loss=0.0099, diff=0.00551, epoch_loss=0.0150, f1-score=0.298, task=1]\u001b[A\n",
      "Batch:  72%|███████▎  | 145/200 [00:27<00:10,  5.45it/s, batch_loss=0.0183, diff=-.00551, epoch_loss=0.0150, f1-score=0.299, task=0]\u001b[A\n",
      "Batch:  73%|███████▎  | 146/200 [00:27<00:09,  5.57it/s, batch_loss=0.0183, diff=-.00551, epoch_loss=0.0150, f1-score=0.299, task=0]\u001b[A\n",
      "Batch:  73%|███████▎  | 146/200 [00:27<00:09,  5.57it/s, batch_loss=0.0099, diff=0.125, epoch_loss=0.0150, f1-score=0.301, task=1]  \u001b[A\n",
      "Batch:  74%|███████▎  | 147/200 [00:27<00:09,  5.62it/s, batch_loss=0.0099, diff=0.125, epoch_loss=0.0150, f1-score=0.301, task=1]\u001b[A\n",
      "Batch:  74%|███████▎  | 147/200 [00:28<00:09,  5.62it/s, batch_loss=0.0098, diff=-.0906, epoch_loss=0.0149, f1-score=0.302, task=1]\u001b[A\n",
      "Batch:  74%|███████▍  | 148/200 [00:28<00:09,  5.25it/s, batch_loss=0.0098, diff=-.0906, epoch_loss=0.0149, f1-score=0.302, task=1]\u001b[A\n",
      "Batch:  74%|███████▍  | 148/200 [00:28<00:09,  5.25it/s, batch_loss=0.0182, diff=-.0346, epoch_loss=0.0150, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  74%|███████▍  | 149/200 [00:28<00:08,  5.67it/s, batch_loss=0.0182, diff=-.0346, epoch_loss=0.0150, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  74%|███████▍  | 149/200 [00:28<00:08,  5.67it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0150, f1-score=0.304, task=0]     \u001b[A\n",
      "Batch:  75%|███████▌  | 150/200 [00:28<00:08,  5.64it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0150, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  75%|███████▌  | 150/200 [00:28<00:08,  5.64it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0149, f1-score=0.302, task=2]\u001b[A\n",
      "Batch:  76%|███████▌  | 151/200 [00:28<00:09,  5.38it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0149, f1-score=0.302, task=2]\u001b[A\n",
      "Batch:  76%|███████▌  | 151/200 [00:28<00:09,  5.38it/s, batch_loss=0.0182, diff=0.426, epoch_loss=0.0150, f1-score=0.303, task=0]\u001b[A\n",
      "Batch:  76%|███████▌  | 152/200 [00:28<00:10,  4.45it/s, batch_loss=0.0182, diff=0.426, epoch_loss=0.0150, f1-score=0.303, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  76%|███████▌  | 152/200 [00:29<00:10,  4.45it/s, batch_loss=0.0098, diff=0.164, epoch_loss=0.0149, f1-score=0.305, task=1]\u001b[A\n",
      "Batch:  76%|███████▋  | 153/200 [00:29<00:10,  4.47it/s, batch_loss=0.0098, diff=0.164, epoch_loss=0.0149, f1-score=0.305, task=1]\u001b[A\n",
      "Batch:  76%|███████▋  | 153/200 [00:29<00:10,  4.47it/s, batch_loss=0.0075, diff=-.59, epoch_loss=0.0149, f1-score=0.303, task=2] \u001b[A\n",
      "Batch:  77%|███████▋  | 154/200 [00:29<00:09,  5.07it/s, batch_loss=0.0075, diff=-.59, epoch_loss=0.0149, f1-score=0.303, task=2]\u001b[A\n",
      "Batch:  77%|███████▋  | 154/200 [00:29<00:09,  5.07it/s, batch_loss=0.0098, diff=0.544, epoch_loss=0.0148, f1-score=0.304, task=1]\u001b[A\n",
      "Batch:  78%|███████▊  | 155/200 [00:29<00:08,  5.37it/s, batch_loss=0.0098, diff=0.544, epoch_loss=0.0148, f1-score=0.304, task=1]\u001b[A\n",
      "Batch:  78%|███████▊  | 155/200 [00:29<00:08,  5.37it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0148, f1-score=0.303, task=2]\u001b[A\n",
      "Batch:  78%|███████▊  | 156/200 [00:29<00:09,  4.77it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0148, f1-score=0.303, task=2]\u001b[A\n",
      "Batch:  78%|███████▊  | 156/200 [00:29<00:09,  4.77it/s, batch_loss=0.0181, diff=0.426, epoch_loss=0.0148, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  78%|███████▊  | 157/200 [00:29<00:08,  4.99it/s, batch_loss=0.0181, diff=0.426, epoch_loss=0.0148, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  78%|███████▊  | 157/200 [00:29<00:08,  4.99it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0148, f1-score=0.304, task=0]    \u001b[A\n",
      "Batch:  79%|███████▉  | 158/200 [00:29<00:08,  5.18it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0148, f1-score=0.304, task=0]\u001b[A\n",
      "Batch:  79%|███████▉  | 158/200 [00:30<00:08,  5.18it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0148, f1-score=0.306, task=1]\u001b[A\n",
      "Batch:  80%|███████▉  | 159/200 [00:30<00:07,  5.36it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0148, f1-score=0.306, task=1]\u001b[A\n",
      "Batch:  80%|███████▉  | 159/200 [00:30<00:07,  5.36it/s, batch_loss=0.0182, diff=-.118, epoch_loss=0.0148, f1-score=0.307, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 160/200 [00:30<00:07,  5.66it/s, batch_loss=0.0182, diff=-.118, epoch_loss=0.0148, f1-score=0.307, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 160/200 [00:30<00:07,  5.66it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0148, f1-score=0.308, task=0]    \u001b[A\n",
      "Batch:  80%|████████  | 161/200 [00:30<00:06,  5.74it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0148, f1-score=0.308, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 161/200 [00:30<00:06,  5.74it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0149, f1-score=0.309, task=0]\u001b[A\n",
      "Batch:  81%|████████  | 162/200 [00:30<00:06,  5.83it/s, batch_loss=0.0182, diff=0, epoch_loss=0.0149, f1-score=0.309, task=0]\u001b[A\n",
      "Batch:  81%|████████  | 162/200 [00:30<00:06,  5.83it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.309, task=0]\u001b[A\n",
      "Batch:  82%|████████▏ | 163/200 [00:30<00:06,  5.87it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.309, task=0]\u001b[A\n",
      "Batch:  82%|████████▏ | 163/200 [00:30<00:06,  5.87it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.31, task=0] \u001b[A\n",
      "Batch:  82%|████████▏ | 164/200 [00:30<00:06,  5.86it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.31, task=0]\u001b[A\n",
      "Batch:  82%|████████▏ | 164/200 [00:31<00:06,  5.86it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.311, task=0]\u001b[A\n",
      "Batch:  82%|████████▎ | 165/200 [00:31<00:05,  5.89it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.311, task=0]\u001b[A\n",
      "Batch:  82%|████████▎ | 165/200 [00:31<00:05,  5.89it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.312, task=0]\u001b[A\n",
      "Batch:  83%|████████▎ | 166/200 [00:31<00:05,  5.82it/s, batch_loss=0.0181, diff=0, epoch_loss=0.0149, f1-score=0.312, task=0]\u001b[A\n",
      "Batch:  83%|████████▎ | 166/200 [00:31<00:05,  5.82it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0149, f1-score=0.313, task=1]\u001b[A\n",
      "Batch:  84%|████████▎ | 167/200 [00:31<00:05,  5.82it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0149, f1-score=0.313, task=1]\u001b[A\n",
      "Batch:  84%|████████▎ | 167/200 [00:31<00:05,  5.82it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0149, f1-score=0.311, task=2]\u001b[A\n",
      "Batch:  84%|████████▍ | 168/200 [00:31<00:05,  5.98it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0149, f1-score=0.311, task=2]\u001b[A\n",
      "Batch:  84%|████████▍ | 168/200 [00:31<00:05,  5.98it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0148, f1-score=0.31, task=2]     \u001b[A\n",
      "Batch:  84%|████████▍ | 169/200 [00:31<00:05,  5.76it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0148, f1-score=0.31, task=2]\u001b[A\n",
      "Batch:  84%|████████▍ | 169/200 [00:32<00:05,  5.76it/s, batch_loss=0.0181, diff=0.426, epoch_loss=0.0148, f1-score=0.311, task=0]\u001b[A\n",
      "Batch:  85%|████████▌ | 170/200 [00:32<00:05,  5.84it/s, batch_loss=0.0181, diff=0.426, epoch_loss=0.0148, f1-score=0.311, task=0]\u001b[A\n",
      "Batch:  85%|████████▌ | 170/200 [00:32<00:05,  5.84it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.309, task=2]\u001b[A\n",
      "Batch:  86%|████████▌ | 171/200 [00:32<00:04,  5.97it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.309, task=2]\u001b[A\n",
      "Batch:  86%|████████▌ | 171/200 [00:32<00:04,  5.97it/s, batch_loss=0.0180, diff=0.426, epoch_loss=0.0148, f1-score=0.31, task=0] \u001b[A\n",
      "Batch:  86%|████████▌ | 172/200 [00:32<00:06,  4.41it/s, batch_loss=0.0180, diff=0.426, epoch_loss=0.0148, f1-score=0.31, task=0]\u001b[A\n",
      "Batch:  86%|████████▌ | 172/200 [00:32<00:06,  4.41it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0148, f1-score=0.311, task=1]\u001b[A\n",
      "Batch:  86%|████████▋ | 173/200 [00:32<00:06,  4.47it/s, batch_loss=0.0098, diff=0.118, epoch_loss=0.0148, f1-score=0.311, task=1]\u001b[A\n",
      "Batch:  86%|████████▋ | 173/200 [00:32<00:06,  4.47it/s, batch_loss=0.0180, diff=-.118, epoch_loss=0.0148, f1-score=0.312, task=0]\u001b[A\n",
      "Batch:  87%|████████▋ | 174/200 [00:32<00:05,  4.67it/s, batch_loss=0.0180, diff=-.118, epoch_loss=0.0148, f1-score=0.312, task=0]\u001b[A\n",
      "Batch:  87%|████████▋ | 174/200 [00:33<00:05,  4.67it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.313, task=0]    \u001b[A\n",
      "Batch:  88%|████████▊ | 175/200 [00:33<00:05,  4.94it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.313, task=0]\u001b[A\n",
      "Batch:  88%|████████▊ | 175/200 [00:33<00:05,  4.94it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0149, f1-score=0.313, task=0]\u001b[A\n",
      "Batch:  88%|████████▊ | 176/200 [00:33<00:05,  4.34it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0149, f1-score=0.313, task=0]\u001b[A\n",
      "Batch:  88%|████████▊ | 176/200 [00:33<00:05,  4.34it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.312, task=2]\u001b[A\n",
      "Batch:  88%|████████▊ | 177/200 [00:33<00:05,  4.25it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.312, task=2]\u001b[A\n",
      "Batch:  88%|████████▊ | 177/200 [00:33<00:05,  4.25it/s, batch_loss=0.0097, diff=0.544, epoch_loss=0.0148, f1-score=0.313, task=1]\u001b[A\n",
      "Batch:  89%|████████▉ | 178/200 [00:33<00:04,  4.58it/s, batch_loss=0.0097, diff=0.544, epoch_loss=0.0148, f1-score=0.313, task=1]\u001b[A\n",
      "Batch:  89%|████████▉ | 178/200 [00:33<00:04,  4.58it/s, batch_loss=0.0180, diff=-.118, epoch_loss=0.0148, f1-score=0.314, task=0]\u001b[A\n",
      "Batch:  90%|████████▉ | 179/200 [00:33<00:04,  5.03it/s, batch_loss=0.0180, diff=-.118, epoch_loss=0.0148, f1-score=0.314, task=0]\u001b[A\n",
      "Batch:  90%|████████▉ | 179/200 [00:34<00:04,  5.03it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.314, task=0]    \u001b[A\n",
      "Batch:  90%|█████████ | 180/200 [00:34<00:04,  4.99it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.314, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 180/200 [00:34<00:04,  4.99it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.315, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 181/200 [00:34<00:04,  4.71it/s, batch_loss=0.0180, diff=0, epoch_loss=0.0148, f1-score=0.315, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 181/200 [00:34<00:04,  4.71it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.316, task=0]\u001b[A\n",
      "Batch:  91%|█████████ | 182/200 [00:34<00:03,  4.57it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.316, task=0]\u001b[A\n",
      "Batch:  91%|█████████ | 182/200 [00:34<00:03,  4.57it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  92%|█████████▏| 183/200 [00:34<00:03,  4.30it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 183/200 [00:35<00:03,  4.30it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 184/200 [00:35<00:03,  4.37it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 184/200 [00:35<00:03,  4.37it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.316, task=2]\u001b[A\n",
      "Batch:  92%|█████████▎| 185/200 [00:35<00:03,  4.81it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0148, f1-score=0.316, task=2]\u001b[A\n",
      "Batch:  92%|█████████▎| 185/200 [00:35<00:03,  4.81it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0149, f1-score=0.316, task=0]\u001b[A\n",
      "Batch:  93%|█████████▎| 186/200 [00:35<00:02,  5.13it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0149, f1-score=0.316, task=0]\u001b[A\n",
      "Batch:  93%|█████████▎| 186/200 [00:35<00:02,  5.13it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]    \u001b[A\n",
      "Batch:  94%|█████████▎| 187/200 [00:35<00:02,  5.36it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.317, task=0]\u001b[A\n",
      "Batch:  94%|█████████▎| 187/200 [00:35<00:02,  5.36it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 188/200 [00:35<00:02,  5.10it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 188/200 [00:36<00:02,  5.10it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 189/200 [00:36<00:02,  4.81it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 189/200 [00:36<00:02,  4.81it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.319, task=0]\u001b[A\n",
      "Batch:  95%|█████████▌| 190/200 [00:36<00:01,  5.08it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0149, f1-score=0.319, task=0]\u001b[A\n",
      "Batch:  95%|█████████▌| 190/200 [00:36<00:01,  5.08it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0149, f1-score=0.317, task=2]\u001b[A\n",
      "Batch:  96%|█████████▌| 191/200 [00:36<00:01,  5.25it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0149, f1-score=0.317, task=2]\u001b[A\n",
      "Batch:  96%|█████████▌| 191/200 [00:36<00:01,  5.25it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  96%|█████████▌| 192/200 [00:36<00:01,  5.38it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0149, f1-score=0.318, task=0]\u001b[A\n",
      "Batch:  96%|█████████▌| 192/200 [00:36<00:01,  5.38it/s, batch_loss=0.0097, diff=0.118, epoch_loss=0.0149, f1-score=0.319, task=1]\u001b[A\n",
      "Batch:  96%|█████████▋| 193/200 [00:36<00:01,  5.42it/s, batch_loss=0.0097, diff=0.118, epoch_loss=0.0149, f1-score=0.319, task=1]\u001b[A\n",
      "Batch:  96%|█████████▋| 193/200 [00:36<00:01,  5.42it/s, batch_loss=0.0097, diff=0, epoch_loss=0.0148, f1-score=0.32, task=1]     \u001b[A\n",
      "Batch:  97%|█████████▋| 194/200 [00:36<00:01,  5.36it/s, batch_loss=0.0097, diff=0, epoch_loss=0.0148, f1-score=0.32, task=1]\u001b[A\n",
      "Batch:  97%|█████████▋| 194/200 [00:37<00:01,  5.36it/s, batch_loss=0.0097, diff=0, epoch_loss=0.0148, f1-score=0.322, task=1]\u001b[A\n",
      "Batch:  98%|█████████▊| 195/200 [00:37<00:00,  5.70it/s, batch_loss=0.0097, diff=0, epoch_loss=0.0148, f1-score=0.322, task=1]\u001b[A\n",
      "Batch:  98%|█████████▊| 195/200 [00:37<00:00,  5.70it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0148, f1-score=0.32, task=2]\u001b[A\n",
      "Batch:  98%|█████████▊| 196/200 [00:37<00:00,  6.05it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0148, f1-score=0.32, task=2]\u001b[A\n",
      "Batch:  98%|█████████▊| 196/200 [00:37<00:00,  6.05it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0148, f1-score=0.321, task=0]\u001b[A\n",
      "Batch:  98%|█████████▊| 197/200 [00:37<00:00,  5.92it/s, batch_loss=0.0179, diff=0.426, epoch_loss=0.0148, f1-score=0.321, task=0]\u001b[A\n",
      "Batch:  98%|█████████▊| 197/200 [00:37<00:00,  5.92it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0148, f1-score=0.321, task=0]    \u001b[A\n",
      "Batch:  99%|█████████▉| 198/200 [00:37<00:00,  4.93it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0148, f1-score=0.321, task=0]\u001b[A\n",
      "Batch:  99%|█████████▉| 198/200 [00:37<00:00,  4.93it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0148, f1-score=0.322, task=0]\u001b[A\n",
      "Batch: 100%|█████████▉| 199/200 [00:37<00:00,  4.65it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0148, f1-score=0.322, task=0]\u001b[A\n",
      "Batch: 100%|█████████▉| 199/200 [00:38<00:00,  4.65it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0148, f1-score=0.323, task=0]\u001b[A\n",
      "Batch: 100%|██████████| 200/200 [00:38<00:00,  4.70it/s, batch_loss=0.0179, diff=0, epoch_loss=0.0148, f1-score=0.323, task=0]\u001b[A\n",
      "Training model:  50%|█████     | 1/2 [00:38<00:38, 38.22s/it, epoch_loss=0.0148]                                              \u001b[A\n",
      "Batch:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 0/200 [00:00<?, ?it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0178, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   0%|          | 1/200 [00:00<00:52,  3.81it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0178, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   0%|          | 1/200 [00:00<00:52,  3.81it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0127, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   1%|          | 2/200 [00:00<00:53,  3.70it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0127, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   1%|          | 2/200 [00:00<00:53,  3.70it/s, batch_loss=0.0097, diff=0.544, epoch_loss=0.0117, f1-score=0.323, task=1]\u001b[A\n",
      "Batch:   2%|▏         | 3/200 [00:00<00:49,  3.97it/s, batch_loss=0.0097, diff=0.544, epoch_loss=0.0117, f1-score=0.323, task=1]\u001b[A\n",
      "Batch:   2%|▏         | 3/200 [00:00<00:49,  3.97it/s, batch_loss=0.0178, diff=-.118, epoch_loss=0.0132, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   2%|▏         | 4/200 [00:00<00:45,  4.29it/s, batch_loss=0.0178, diff=-.118, epoch_loss=0.0132, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   2%|▏         | 4/200 [00:01<00:45,  4.29it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0121, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   2%|▎         | 5/200 [00:01<00:47,  4.14it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0121, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   2%|▎         | 5/200 [00:01<00:47,  4.14it/s, batch_loss=0.0178, diff=0.426, epoch_loss=0.0130, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   3%|▎         | 6/200 [00:01<00:43,  4.45it/s, batch_loss=0.0178, diff=0.426, epoch_loss=0.0130, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   3%|▎         | 6/200 [00:01<00:43,  4.45it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0137, f1-score=0.323, task=0]    \u001b[A\n",
      "Batch:   4%|▎         | 7/200 [00:01<00:46,  4.12it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0137, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   4%|▎         | 7/200 [00:01<00:46,  4.12it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0142, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:   4%|▍         | 8/200 [00:01<00:43,  4.43it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0142, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:   4%|▍         | 8/200 [00:02<00:43,  4.43it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0135, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   4%|▍         | 9/200 [00:02<00:42,  4.46it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0135, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   4%|▍         | 9/200 [00:02<00:42,  4.46it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0129, f1-score=0.321, task=2]    \u001b[A\n",
      "Batch:   5%|▌         | 10/200 [00:02<00:38,  4.97it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0129, f1-score=0.321, task=2]\u001b[A\n",
      "Batch:   5%|▌         | 10/200 [00:02<00:38,  4.97it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0124, f1-score=0.319, task=2]\u001b[A\n",
      "Batch:   6%|▌         | 11/200 [00:02<00:39,  4.81it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0124, f1-score=0.319, task=2]\u001b[A\n",
      "Batch:   6%|▌         | 11/200 [00:02<00:39,  4.81it/s, batch_loss=0.0096, diff=0.544, epoch_loss=0.0122, f1-score=0.32, task=1]\u001b[A\n",
      "Batch:   6%|▌         | 12/200 [00:02<00:37,  4.96it/s, batch_loss=0.0096, diff=0.544, epoch_loss=0.0122, f1-score=0.32, task=1]\u001b[A\n",
      "Batch:   6%|▌         | 12/200 [00:02<00:37,  4.96it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0120, f1-score=0.322, task=1]   \u001b[A\n",
      "Batch:   6%|▋         | 13/200 [00:02<00:36,  5.07it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0120, f1-score=0.322, task=1]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   6%|▋         | 13/200 [00:03<00:36,  5.07it/s, batch_loss=0.0178, diff=-.118, epoch_loss=0.0124, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:   7%|▋         | 14/200 [00:03<00:43,  4.31it/s, batch_loss=0.0178, diff=-.118, epoch_loss=0.0124, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:   7%|▋         | 14/200 [00:03<00:43,  4.31it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0127, f1-score=0.323, task=0]    \u001b[A\n",
      "Batch:   8%|▊         | 15/200 [00:03<00:43,  4.26it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0127, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 15/200 [00:03<00:43,  4.26it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0130, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 16/200 [00:03<00:39,  4.65it/s, batch_loss=0.0178, diff=0, epoch_loss=0.0130, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:   8%|▊         | 16/200 [00:03<00:39,  4.65it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0127, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   8%|▊         | 17/200 [00:03<00:36,  5.04it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0127, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:   8%|▊         | 17/200 [00:03<00:36,  5.04it/s, batch_loss=0.0178, diff=0.426, epoch_loss=0.0130, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:   9%|▉         | 18/200 [00:03<00:34,  5.29it/s, batch_loss=0.0178, diff=0.426, epoch_loss=0.0130, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:   9%|▉         | 18/200 [00:04<00:34,  5.29it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0133, f1-score=0.323, task=0]    \u001b[A\n",
      "Batch:  10%|▉         | 19/200 [00:04<00:34,  5.21it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0133, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  10%|▉         | 19/200 [00:04<00:34,  5.21it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0135, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  10%|█         | 20/200 [00:04<00:39,  4.54it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0135, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  10%|█         | 20/200 [00:04<00:39,  4.54it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0132, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:  10%|█         | 21/200 [00:04<00:36,  4.85it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0132, f1-score=0.322, task=2]\u001b[A\n",
      "Batch:  10%|█         | 21/200 [00:04<00:36,  4.85it/s, batch_loss=0.0177, diff=0.426, epoch_loss=0.0134, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  11%|█         | 22/200 [00:04<00:33,  5.38it/s, batch_loss=0.0177, diff=0.426, epoch_loss=0.0134, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  11%|█         | 22/200 [00:04<00:33,  5.38it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0131, f1-score=0.321, task=2]\u001b[A\n",
      "Batch:  12%|█▏        | 23/200 [00:04<00:36,  4.81it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0131, f1-score=0.321, task=2]\u001b[A\n",
      "Batch:  12%|█▏        | 23/200 [00:05<00:36,  4.81it/s, batch_loss=0.0177, diff=0.426, epoch_loss=0.0133, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:  12%|█▏        | 24/200 [00:05<00:36,  4.79it/s, batch_loss=0.0177, diff=0.426, epoch_loss=0.0133, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:  12%|█▏        | 24/200 [00:05<00:36,  4.79it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0135, f1-score=0.322, task=0]    \u001b[A\n",
      "Batch:  12%|█▎        | 25/200 [00:05<00:34,  5.05it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0135, f1-score=0.322, task=0]\u001b[A\n",
      "Batch:  12%|█▎        | 25/200 [00:05<00:34,  5.05it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0137, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  13%|█▎        | 26/200 [00:05<00:31,  5.46it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0137, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  13%|█▎        | 26/200 [00:05<00:31,  5.46it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0138, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  14%|█▎        | 27/200 [00:05<00:32,  5.39it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0138, f1-score=0.323, task=0]\u001b[A\n",
      "Batch:  14%|█▎        | 27/200 [00:05<00:32,  5.39it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0140, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 28/200 [00:05<00:36,  4.73it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0140, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 28/200 [00:06<00:36,  4.73it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0141, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 29/200 [00:06<00:36,  4.67it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0141, f1-score=0.324, task=0]\u001b[A\n",
      "Batch:  14%|█▍        | 29/200 [00:06<00:36,  4.67it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0142, f1-score=0.325, task=0]\u001b[A\n",
      "Batch:  15%|█▌        | 30/200 [00:06<00:35,  4.84it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0142, f1-score=0.325, task=0]\u001b[A\n",
      "Batch:  15%|█▌        | 30/200 [00:06<00:35,  4.84it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0143, f1-score=0.325, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 31/200 [00:06<00:33,  4.99it/s, batch_loss=0.0177, diff=0, epoch_loss=0.0143, f1-score=0.325, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 31/200 [00:06<00:33,  4.99it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0144, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 32/200 [00:06<00:32,  5.12it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0144, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  16%|█▌        | 32/200 [00:06<00:32,  5.12it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0145, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  16%|█▋        | 33/200 [00:06<00:35,  4.77it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0145, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  16%|█▋        | 33/200 [00:07<00:35,  4.77it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0143, f1-score=0.325, task=2]\u001b[A\n",
      "Batch:  17%|█▋        | 34/200 [00:07<00:35,  4.70it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0143, f1-score=0.325, task=2]\u001b[A\n",
      "Batch:  17%|█▋        | 34/200 [00:07<00:35,  4.70it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0144, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  18%|█▊        | 35/200 [00:07<00:33,  4.95it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0144, f1-score=0.326, task=0]\u001b[A\n",
      "Batch:  18%|█▊        | 35/200 [00:07<00:33,  4.95it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.324, task=2]\u001b[A\n",
      "Batch:  18%|█▊        | 36/200 [00:07<00:32,  4.99it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.324, task=2]\u001b[A\n",
      "Batch:  18%|█▊        | 36/200 [00:07<00:32,  4.99it/s, batch_loss=0.0096, diff=0.544, epoch_loss=0.0141, f1-score=0.325, task=1]\u001b[A\n",
      "Batch:  18%|█▊        | 37/200 [00:07<00:36,  4.50it/s, batch_loss=0.0096, diff=0.544, epoch_loss=0.0141, f1-score=0.325, task=1]\u001b[A\n",
      "Batch:  18%|█▊        | 37/200 [00:08<00:36,  4.50it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0140, f1-score=0.326, task=1]    \u001b[A\n",
      "Batch:  19%|█▉        | 38/200 [00:08<00:36,  4.50it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0140, f1-score=0.326, task=1]\u001b[A\n",
      "Batch:  19%|█▉        | 38/200 [00:08<00:36,  4.50it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0139, f1-score=0.327, task=1]\u001b[A\n",
      "Batch:  20%|█▉        | 39/200 [00:08<00:34,  4.68it/s, batch_loss=0.0096, diff=0, epoch_loss=0.0139, f1-score=0.327, task=1]\u001b[A\n",
      "Batch:  20%|█▉        | 39/200 [00:08<00:34,  4.68it/s, batch_loss=0.0176, diff=-.118, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  20%|██        | 40/200 [00:08<00:34,  4.69it/s, batch_loss=0.0176, diff=-.118, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  20%|██        | 40/200 [00:08<00:34,  4.69it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.328, task=0]    \u001b[A\n",
      "Batch:  20%|██        | 41/200 [00:08<00:32,  4.88it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  20%|██        | 41/200 [00:08<00:32,  4.88it/s, batch_loss=0.0096, diff=0.118, epoch_loss=0.0139, f1-score=0.329, task=1]\u001b[A\n",
      "Batch:  21%|██        | 42/200 [00:08<00:32,  4.82it/s, batch_loss=0.0096, diff=0.118, epoch_loss=0.0139, f1-score=0.329, task=1]\u001b[A\n",
      "Batch:  21%|██        | 42/200 [00:09<00:32,  4.82it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0138, f1-score=0.328, task=2]\u001b[A\n",
      "Batch:  22%|██▏       | 43/200 [00:09<00:30,  5.15it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0138, f1-score=0.328, task=2]\u001b[A\n",
      "Batch:  22%|██▏       | 43/200 [00:09<00:30,  5.15it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  22%|██▏       | 44/200 [00:09<00:34,  4.47it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  22%|██▏       | 44/200 [00:09<00:34,  4.47it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.329, task=0]    \u001b[A\n",
      "Batch:  22%|██▎       | 45/200 [00:09<00:35,  4.43it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.329, task=0]\u001b[A\n",
      "Batch:  22%|██▎       | 45/200 [00:09<00:35,  4.43it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0138, f1-score=0.328, task=2]\u001b[A\n",
      "Batch:  23%|██▎       | 46/200 [00:09<00:33,  4.59it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0138, f1-score=0.328, task=2]\u001b[A\n",
      "Batch:  23%|██▎       | 46/200 [00:09<00:33,  4.59it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  24%|██▎       | 47/200 [00:09<00:30,  4.99it/s, batch_loss=0.0176, diff=0.426, epoch_loss=0.0139, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  24%|██▎       | 47/200 [00:10<00:30,  4.99it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.328, task=0]    \u001b[A\n",
      "Batch:  24%|██▍       | 48/200 [00:10<00:30,  4.97it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0140, f1-score=0.328, task=0]\u001b[A\n",
      "Batch:  24%|██▍       | 48/200 [00:10<00:30,  4.97it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0140, f1-score=0.329, task=0]\u001b[A\n",
      "Batch:  24%|██▍       | 49/200 [00:10<00:31,  4.72it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0140, f1-score=0.329, task=0]\u001b[A\n",
      "Batch:  24%|██▍       | 49/200 [00:10<00:31,  4.72it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0141, f1-score=0.329, task=0]\u001b[A\n",
      "Batch:  25%|██▌       | 50/200 [00:10<00:29,  5.02it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0141, f1-score=0.329, task=0]\u001b[A\n",
      "Batch:  25%|██▌       | 50/200 [00:10<00:29,  5.02it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.33, task=0] \u001b[A\n",
      "Batch:  26%|██▌       | 51/200 [00:10<00:28,  5.16it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  26%|██▌       | 51/200 [00:10<00:28,  5.16it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0142, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  26%|██▌       | 52/200 [00:10<00:28,  5.27it/s, batch_loss=0.0176, diff=0, epoch_loss=0.0142, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  26%|██▌       | 52/200 [00:11<00:28,  5.27it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0142, f1-score=0.331, task=1]\u001b[A\n",
      "Batch:  26%|██▋       | 53/200 [00:11<00:31,  4.71it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0142, f1-score=0.331, task=1]\u001b[A\n",
      "Batch:  26%|██▋       | 53/200 [00:11<00:31,  4.71it/s, batch_loss=0.0175, diff=-.118, epoch_loss=0.0142, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  27%|██▋       | 54/200 [00:11<00:30,  4.85it/s, batch_loss=0.0175, diff=-.118, epoch_loss=0.0142, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  27%|██▋       | 54/200 [00:11<00:30,  4.85it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]    \u001b[A\n",
      "Batch:  28%|██▊       | 55/200 [00:11<00:29,  4.93it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 55/200 [00:11<00:29,  4.93it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 56/200 [00:11<00:29,  4.82it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 56/200 [00:11<00:29,  4.82it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0144, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 57/200 [00:11<00:29,  4.79it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0144, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  28%|██▊       | 57/200 [00:12<00:29,  4.79it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0144, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  29%|██▉       | 58/200 [00:12<00:28,  5.01it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0144, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  29%|██▉       | 58/200 [00:12<00:28,  5.01it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0145, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  30%|██▉       | 59/200 [00:12<00:27,  5.08it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0145, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  30%|██▉       | 59/200 [00:12<00:27,  5.08it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0144, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  30%|███       | 60/200 [00:12<00:26,  5.29it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0144, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  30%|███       | 60/200 [00:12<00:26,  5.29it/s, batch_loss=0.0095, diff=0.544, epoch_loss=0.0143, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  30%|███       | 61/200 [00:12<00:25,  5.44it/s, batch_loss=0.0095, diff=0.544, epoch_loss=0.0143, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  30%|███       | 61/200 [00:12<00:25,  5.44it/s, batch_loss=0.0175, diff=-.118, epoch_loss=0.0144, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  31%|███       | 62/200 [00:12<00:23,  5.80it/s, batch_loss=0.0175, diff=-.118, epoch_loss=0.0144, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  31%|███       | 62/200 [00:13<00:23,  5.80it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  32%|███▏      | 63/200 [00:13<00:29,  4.65it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  32%|███▏      | 63/200 [00:13<00:29,  4.65it/s, batch_loss=0.0175, diff=0.426, epoch_loss=0.0143, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  32%|███▏      | 64/200 [00:13<00:30,  4.45it/s, batch_loss=0.0175, diff=0.426, epoch_loss=0.0143, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  32%|███▏      | 64/200 [00:13<00:30,  4.45it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.333, task=0]    \u001b[A\n",
      "Batch:  32%|███▎      | 65/200 [00:13<00:28,  4.71it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  32%|███▎      | 65/200 [00:13<00:28,  4.71it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  33%|███▎      | 66/200 [00:13<00:27,  4.85it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  33%|███▎      | 66/200 [00:13<00:27,  4.85it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.331, task=2]    \u001b[A\n",
      "Batch:  34%|███▎      | 67/200 [00:13<00:27,  4.76it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.331, task=2]\u001b[A\n",
      "Batch:  34%|███▎      | 67/200 [00:14<00:27,  4.76it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0140, f1-score=0.33, task=2] \u001b[A\n",
      "Batch:  34%|███▍      | 68/200 [00:14<00:25,  5.16it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0140, f1-score=0.33, task=2]\u001b[A\n",
      "Batch:  34%|███▍      | 68/200 [00:14<00:25,  5.16it/s, batch_loss=0.0175, diff=0.426, epoch_loss=0.0141, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  34%|███▍      | 69/200 [00:14<00:28,  4.60it/s, batch_loss=0.0175, diff=0.426, epoch_loss=0.0141, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  34%|███▍      | 69/200 [00:14<00:28,  4.60it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0141, f1-score=0.331, task=0]   \u001b[A\n",
      "Batch:  35%|███▌      | 70/200 [00:14<00:28,  4.62it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0141, f1-score=0.331, task=0]\u001b[A\n",
      "Batch:  35%|███▌      | 70/200 [00:14<00:28,  4.62it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.331, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 71/200 [00:14<00:27,  4.65it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.331, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 71/200 [00:15<00:27,  4.65it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 72/200 [00:15<00:26,  4.83it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0142, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  36%|███▌      | 72/200 [00:15<00:26,  4.83it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  36%|███▋      | 73/200 [00:15<00:30,  4.21it/s, batch_loss=0.0175, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  36%|███▋      | 73/200 [00:15<00:30,  4.21it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  37%|███▋      | 74/200 [00:15<00:27,  4.50it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0143, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  37%|███▋      | 74/200 [00:15<00:27,  4.50it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0143, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  38%|███▊      | 75/200 [00:15<00:26,  4.72it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0143, f1-score=0.333, task=1]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  38%|███▊      | 75/200 [00:15<00:26,  4.72it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0143, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 76/200 [00:15<00:28,  4.38it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0143, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 76/200 [00:16<00:28,  4.38it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0143, f1-score=0.334, task=0]    \u001b[A\n",
      "Batch:  38%|███▊      | 77/200 [00:16<00:26,  4.57it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0143, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  38%|███▊      | 77/200 [00:16<00:26,  4.57it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  39%|███▉      | 78/200 [00:16<00:24,  4.99it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0142, f1-score=0.333, task=2]\u001b[A\n",
      "Batch:  39%|███▉      | 78/200 [00:16<00:24,  4.99it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0142, f1-score=0.332, task=2]    \u001b[A\n",
      "Batch:  40%|███▉      | 79/200 [00:16<00:23,  5.26it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0142, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  40%|███▉      | 79/200 [00:16<00:23,  5.26it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.331, task=2]\u001b[A\n",
      "Batch:  40%|████      | 80/200 [00:16<00:24,  4.93it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.331, task=2]\u001b[A\n",
      "Batch:  40%|████      | 80/200 [00:16<00:24,  4.93it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0140, f1-score=0.329, task=2]\u001b[A\n",
      "Batch:  40%|████      | 81/200 [00:16<00:23,  5.00it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0140, f1-score=0.329, task=2]\u001b[A\n",
      "Batch:  40%|████      | 81/200 [00:17<00:23,  5.00it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0140, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  41%|████      | 82/200 [00:17<00:22,  5.22it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0140, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  41%|████      | 82/200 [00:17<00:22,  5.22it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0141, f1-score=0.33, task=0]    \u001b[A\n",
      "Batch:  42%|████▏     | 83/200 [00:17<00:22,  5.31it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0141, f1-score=0.33, task=0]\u001b[A\n",
      "Batch:  42%|████▏     | 83/200 [00:17<00:22,  5.31it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0140, f1-score=0.331, task=1]\u001b[A\n",
      "Batch:  42%|████▏     | 84/200 [00:17<00:21,  5.37it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0140, f1-score=0.331, task=1]\u001b[A\n",
      "Batch:  42%|████▏     | 84/200 [00:17<00:21,  5.37it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0141, f1-score=0.331, task=0]\u001b[A\n",
      "Batch:  42%|████▎     | 85/200 [00:17<00:20,  5.53it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0141, f1-score=0.331, task=0]\u001b[A\n",
      "Batch:  42%|████▎     | 85/200 [00:17<00:20,  5.53it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0140, f1-score=0.332, task=1]\u001b[A\n",
      "Batch:  43%|████▎     | 86/200 [00:17<00:20,  5.44it/s, batch_loss=0.0095, diff=0.118, epoch_loss=0.0140, f1-score=0.332, task=1]\u001b[A\n",
      "Batch:  43%|████▎     | 86/200 [00:18<00:20,  5.44it/s, batch_loss=0.0095, diff=0, epoch_loss=0.0140, f1-score=0.333, task=1]    \u001b[A\n",
      "Batch:  44%|████▎     | 87/200 [00:18<00:22,  5.00it/s, batch_loss=0.0095, diff=0, epoch_loss=0.0140, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  44%|████▎     | 87/200 [00:18<00:22,  5.00it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0140, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 88/200 [00:18<00:20,  5.45it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0140, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  44%|████▍     | 88/200 [00:18<00:20,  5.45it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  44%|████▍     | 89/200 [00:18<00:23,  4.80it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  44%|████▍     | 89/200 [00:18<00:23,  4.80it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0140, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  45%|████▌     | 90/200 [00:18<00:25,  4.39it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0140, f1-score=0.333, task=0]\u001b[A\n",
      "Batch:  45%|████▌     | 90/200 [00:18<00:25,  4.39it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  46%|████▌     | 91/200 [00:18<00:22,  4.84it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  46%|████▌     | 91/200 [00:19<00:22,  4.84it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0139, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  46%|████▌     | 92/200 [00:19<00:21,  5.06it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0139, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  46%|████▌     | 92/200 [00:19<00:21,  5.06it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0140, f1-score=0.332, task=0]    \u001b[A\n",
      "Batch:  46%|████▋     | 93/200 [00:19<00:20,  5.17it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0140, f1-score=0.332, task=0]\u001b[A\n",
      "Batch:  46%|████▋     | 93/200 [00:19<00:20,  5.17it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0139, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  47%|████▋     | 94/200 [00:19<00:23,  4.43it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0139, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  47%|████▋     | 94/200 [00:19<00:23,  4.43it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  48%|████▊     | 95/200 [00:19<00:21,  4.78it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0139, f1-score=0.332, task=2]\u001b[A\n",
      "Batch:  48%|████▊     | 95/200 [00:19<00:21,  4.78it/s, batch_loss=0.0094, diff=0.544, epoch_loss=0.0138, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 96/200 [00:19<00:22,  4.66it/s, batch_loss=0.0094, diff=0.544, epoch_loss=0.0138, f1-score=0.333, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 96/200 [00:20<00:22,  4.66it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0138, f1-score=0.334, task=1]    \u001b[A\n",
      "Batch:  48%|████▊     | 97/200 [00:20<00:22,  4.66it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0138, f1-score=0.334, task=1]\u001b[A\n",
      "Batch:  48%|████▊     | 97/200 [00:20<00:22,  4.66it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0137, f1-score=0.334, task=1]\u001b[A\n",
      "Batch:  49%|████▉     | 98/200 [00:20<00:24,  4.19it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0137, f1-score=0.334, task=1]\u001b[A\n",
      "Batch:  49%|████▉     | 98/200 [00:20<00:24,  4.19it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  50%|████▉     | 99/200 [00:20<00:23,  4.25it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  50%|████▉     | 99/200 [00:20<00:23,  4.25it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]    \u001b[A\n",
      "Batch:  50%|█████     | 100/200 [00:20<00:22,  4.50it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  50%|█████     | 100/200 [00:21<00:22,  4.50it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0137, f1-score=0.334, task=2]\u001b[A\n",
      "Batch:  50%|█████     | 101/200 [00:21<00:21,  4.53it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0137, f1-score=0.334, task=2]\u001b[A\n",
      "Batch:  50%|█████     | 101/200 [00:21<00:21,  4.53it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0138, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  51%|█████     | 102/200 [00:21<00:23,  4.11it/s, batch_loss=0.0174, diff=0.426, epoch_loss=0.0138, f1-score=0.334, task=0]\u001b[A\n",
      "Batch:  51%|█████     | 102/200 [00:21<00:23,  4.11it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]    \u001b[A\n",
      "Batch:  52%|█████▏    | 103/200 [00:21<00:22,  4.24it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  52%|█████▏    | 103/200 [00:21<00:22,  4.24it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  52%|█████▏    | 104/200 [00:21<00:22,  4.29it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0138, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  52%|█████▏    | 104/200 [00:22<00:22,  4.29it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  52%|█████▎    | 105/200 [00:22<00:20,  4.55it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.335, task=0]\u001b[A\n",
      "Batch:  52%|█████▎    | 105/200 [00:22<00:20,  4.55it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.336, task=0]\u001b[A\n",
      "Batch:  53%|█████▎    | 106/200 [00:22<00:21,  4.29it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.336, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  53%|█████▎    | 106/200 [00:22<00:21,  4.29it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0139, f1-score=0.337, task=1]\u001b[A\n",
      "Batch:  54%|█████▎    | 107/200 [00:22<00:22,  4.21it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0139, f1-score=0.337, task=1]\u001b[A\n",
      "Batch:  54%|█████▎    | 107/200 [00:22<00:22,  4.21it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0138, f1-score=0.337, task=1]    \u001b[A\n",
      "Batch:  54%|█████▍    | 108/200 [00:22<00:20,  4.50it/s, batch_loss=0.0094, diff=0, epoch_loss=0.0138, f1-score=0.337, task=1]\u001b[A\n",
      "Batch:  54%|█████▍    | 108/200 [00:22<00:20,  4.50it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0138, f1-score=0.338, task=0]\u001b[A\n",
      "Batch:  55%|█████▍    | 109/200 [00:22<00:18,  4.85it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0138, f1-score=0.338, task=0]\u001b[A\n",
      "Batch:  55%|█████▍    | 109/200 [00:23<00:18,  4.85it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0138, f1-score=0.338, task=1]\u001b[A\n",
      "Batch:  55%|█████▌    | 110/200 [00:23<00:17,  5.11it/s, batch_loss=0.0094, diff=0.118, epoch_loss=0.0138, f1-score=0.338, task=1]\u001b[A\n",
      "Batch:  55%|█████▌    | 110/200 [00:23<00:17,  5.11it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0138, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 111/200 [00:23<00:15,  5.66it/s, batch_loss=0.0174, diff=-.118, epoch_loss=0.0138, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 111/200 [00:23<00:15,  5.66it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]    \u001b[A\n",
      "Batch:  56%|█████▌    | 112/200 [00:23<00:18,  4.77it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  56%|█████▌    | 112/200 [00:23<00:18,  4.77it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  56%|█████▋    | 113/200 [00:23<00:16,  5.12it/s, batch_loss=0.0174, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  56%|█████▋    | 113/200 [00:23<00:16,  5.12it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0138, f1-score=0.338, task=2]\u001b[A\n",
      "Batch:  57%|█████▋    | 114/200 [00:23<00:16,  5.16it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0138, f1-score=0.338, task=2]\u001b[A\n",
      "Batch:  57%|█████▋    | 114/200 [00:24<00:16,  5.16it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  57%|█████▊    | 115/200 [00:24<00:16,  5.02it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  57%|█████▊    | 115/200 [00:24<00:16,  5.02it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]    \u001b[A\n",
      "Batch:  58%|█████▊    | 116/200 [00:24<00:17,  4.80it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  58%|█████▊    | 116/200 [00:24<00:17,  4.80it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  58%|█████▊    | 117/200 [00:24<00:16,  5.00it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  58%|█████▊    | 117/200 [00:24<00:16,  5.00it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0] \u001b[A\n",
      "Batch:  59%|█████▉    | 118/200 [00:24<00:15,  5.14it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0]\u001b[A\n",
      "Batch:  59%|█████▉    | 118/200 [00:24<00:15,  5.14it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.339, task=2]\u001b[A\n",
      "Batch:  60%|█████▉    | 119/200 [00:24<00:15,  5.23it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.339, task=2]\u001b[A\n",
      "Batch:  60%|█████▉    | 119/200 [00:24<00:15,  5.23it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  60%|██████    | 120/200 [00:24<00:14,  5.35it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  60%|██████    | 120/200 [00:25<00:14,  5.35it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]    \u001b[A\n",
      "Batch:  60%|██████    | 121/200 [00:25<00:17,  4.45it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  60%|██████    | 121/200 [00:25<00:17,  4.45it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.338, task=2]\u001b[A\n",
      "Batch:  61%|██████    | 122/200 [00:25<00:17,  4.43it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.338, task=2]\u001b[A\n",
      "Batch:  61%|██████    | 122/200 [00:25<00:17,  4.43it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 123/200 [00:25<00:16,  4.79it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0139, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 123/200 [00:25<00:16,  4.79it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]    \u001b[A\n",
      "Batch:  62%|██████▏   | 124/200 [00:25<00:17,  4.34it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  62%|██████▏   | 124/200 [00:26<00:17,  4.34it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  62%|██████▎   | 125/200 [00:26<00:17,  4.30it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.339, task=0]\u001b[A\n",
      "Batch:  62%|██████▎   | 125/200 [00:26<00:17,  4.30it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0] \u001b[A\n",
      "Batch:  63%|██████▎   | 126/200 [00:26<00:16,  4.38it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0]\u001b[A\n",
      "Batch:  63%|██████▎   | 126/200 [00:26<00:16,  4.38it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0]\u001b[A\n",
      "Batch:  64%|██████▎   | 127/200 [00:26<00:15,  4.57it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0140, f1-score=0.34, task=0]\u001b[A\n",
      "Batch:  64%|██████▎   | 127/200 [00:26<00:15,  4.57it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0140, f1-score=0.34, task=1]\u001b[A\n",
      "Batch:  64%|██████▍   | 128/200 [00:26<00:16,  4.26it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0140, f1-score=0.34, task=1]\u001b[A\n",
      "Batch:  64%|██████▍   | 128/200 [00:27<00:16,  4.26it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0140, f1-score=0.341, task=0]\u001b[A\n",
      "Batch:  64%|██████▍   | 129/200 [00:27<00:15,  4.51it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0140, f1-score=0.341, task=0]\u001b[A\n",
      "Batch:  64%|██████▍   | 129/200 [00:27<00:15,  4.51it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.341, task=0]    \u001b[A\n",
      "Batch:  65%|██████▌   | 130/200 [00:27<00:14,  4.74it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.341, task=0]\u001b[A\n",
      "Batch:  65%|██████▌   | 130/200 [00:27<00:14,  4.74it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.341, task=0]\u001b[A\n",
      "Batch:  66%|██████▌   | 131/200 [00:27<00:13,  4.97it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.341, task=0]\u001b[A\n",
      "Batch:  66%|██████▌   | 131/200 [00:27<00:13,  4.97it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0140, f1-score=0.342, task=1]\u001b[A\n",
      "Batch:  66%|██████▌   | 132/200 [00:27<00:13,  5.07it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0140, f1-score=0.342, task=1]\u001b[A\n",
      "Batch:  66%|██████▌   | 132/200 [00:27<00:13,  5.07it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  66%|██████▋   | 133/200 [00:27<00:12,  5.30it/s, batch_loss=0.0173, diff=-.118, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  66%|██████▋   | 133/200 [00:27<00:12,  5.30it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0140, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  67%|██████▋   | 134/200 [00:27<00:12,  5.33it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0140, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  67%|██████▋   | 134/200 [00:28<00:12,  5.33it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0140, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 135/200 [00:28<00:13,  4.73it/s, batch_loss=0.0173, diff=0.426, epoch_loss=0.0140, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 135/200 [00:28<00:13,  4.73it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]    \u001b[A\n",
      "Batch:  68%|██████▊   | 136/200 [00:28<00:17,  3.65it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 136/200 [00:28<00:17,  3.65it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  68%|██████▊   | 137/200 [00:28<00:17,  3.67it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  68%|██████▊   | 137/200 [00:29<00:17,  3.67it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  69%|██████▉   | 138/200 [00:29<00:15,  3.92it/s, batch_loss=0.0173, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  69%|██████▉   | 138/200 [00:29<00:15,  3.92it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  70%|██████▉   | 139/200 [00:29<00:14,  4.14it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  70%|██████▉   | 139/200 [00:29<00:14,  4.14it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  70%|███████   | 140/200 [00:29<00:13,  4.54it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  70%|███████   | 140/200 [00:29<00:13,  4.54it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.344, task=1]\u001b[A\n",
      "Batch:  70%|███████   | 141/200 [00:29<00:12,  4.86it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.344, task=1]\u001b[A\n",
      "Batch:  70%|███████   | 141/200 [00:29<00:12,  4.86it/s, batch_loss=0.0172, diff=-.118, epoch_loss=0.0141, f1-score=0.344, task=0]\u001b[A\n",
      "Batch:  71%|███████   | 142/200 [00:29<00:10,  5.48it/s, batch_loss=0.0172, diff=-.118, epoch_loss=0.0141, f1-score=0.344, task=0]\u001b[A\n",
      "Batch:  71%|███████   | 142/200 [00:30<00:10,  5.48it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.344, task=0]    \u001b[A\n",
      "Batch:  72%|███████▏  | 143/200 [00:30<00:11,  4.84it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.344, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 143/200 [00:30<00:11,  4.84it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 144/200 [00:30<00:11,  5.08it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0142, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  72%|███████▏  | 144/200 [00:30<00:11,  5.08it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.344, task=2]\u001b[A\n",
      "Batch:  72%|███████▎  | 145/200 [00:30<00:10,  5.12it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.344, task=2]\u001b[A\n",
      "Batch:  72%|███████▎  | 145/200 [00:30<00:10,  5.12it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.343, task=2]    \u001b[A\n",
      "Batch:  73%|███████▎  | 146/200 [00:30<00:11,  4.90it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.343, task=2]\u001b[A\n",
      "Batch:  73%|███████▎  | 146/200 [00:30<00:11,  4.90it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.342, task=2]\u001b[A\n",
      "Batch:  74%|███████▎  | 147/200 [00:30<00:11,  4.59it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0141, f1-score=0.342, task=2]\u001b[A\n",
      "Batch:  74%|███████▎  | 147/200 [00:31<00:11,  4.59it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  74%|███████▍  | 148/200 [00:31<00:10,  4.73it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  74%|███████▍  | 148/200 [00:31<00:10,  4.73it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]    \u001b[A\n",
      "Batch:  74%|███████▍  | 149/200 [00:31<00:10,  4.79it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  74%|███████▍  | 149/200 [00:31<00:10,  4.79it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  75%|███████▌  | 150/200 [00:31<00:10,  4.63it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  75%|███████▌  | 150/200 [00:31<00:10,  4.63it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  76%|███████▌  | 151/200 [00:31<00:11,  4.28it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  76%|███████▌  | 151/200 [00:32<00:11,  4.28it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]    \u001b[A\n",
      "Batch:  76%|███████▌  | 152/200 [00:32<00:10,  4.55it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  76%|███████▌  | 152/200 [00:32<00:10,  4.55it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  76%|███████▋  | 153/200 [00:32<00:09,  4.79it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  76%|███████▋  | 153/200 [00:32<00:09,  4.79it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  77%|███████▋  | 154/200 [00:32<00:09,  5.03it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0141, f1-score=0.341, task=2]\u001b[A\n",
      "Batch:  77%|███████▋  | 154/200 [00:32<00:09,  5.03it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  78%|███████▊  | 155/200 [00:32<00:08,  5.47it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  78%|███████▊  | 155/200 [00:32<00:08,  5.47it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]    \u001b[A\n",
      "Batch:  78%|███████▊  | 156/200 [00:32<00:09,  4.60it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  78%|███████▊  | 156/200 [00:33<00:09,  4.60it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.343, task=1]\u001b[A\n",
      "Batch:  78%|███████▊  | 157/200 [00:33<00:08,  4.83it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.343, task=1]\u001b[A\n",
      "Batch:  78%|███████▊  | 157/200 [00:33<00:08,  4.83it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0140, f1-score=0.342, task=2]\u001b[A\n",
      "Batch:  79%|███████▉  | 158/200 [00:33<00:08,  5.11it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0140, f1-score=0.342, task=2]\u001b[A\n",
      "Batch:  79%|███████▉  | 158/200 [00:33<00:08,  5.11it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  80%|███████▉  | 159/200 [00:33<00:09,  4.52it/s, batch_loss=0.0172, diff=0.426, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  80%|███████▉  | 159/200 [00:33<00:09,  4.52it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]    \u001b[A\n",
      "Batch:  80%|████████  | 160/200 [00:33<00:08,  4.68it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.342, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 160/200 [00:33<00:08,  4.68it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 161/200 [00:33<00:07,  4.92it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  80%|████████  | 161/200 [00:34<00:07,  4.92it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  81%|████████  | 162/200 [00:34<00:08,  4.74it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0141, f1-score=0.343, task=0]\u001b[A\n",
      "Batch:  81%|████████  | 162/200 [00:34<00:08,  4.74it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.343, task=1]\u001b[A\n",
      "Batch:  82%|████████▏ | 163/200 [00:34<00:08,  4.33it/s, batch_loss=0.0093, diff=0.118, epoch_loss=0.0141, f1-score=0.343, task=1]\u001b[A\n",
      "Batch:  82%|████████▏ | 163/200 [00:34<00:08,  4.33it/s, batch_loss=0.0093, diff=0, epoch_loss=0.0141, f1-score=0.344, task=1]    \u001b[A\n",
      "Batch:  82%|████████▏ | 164/200 [00:34<00:08,  4.38it/s, batch_loss=0.0093, diff=0, epoch_loss=0.0141, f1-score=0.344, task=1]\u001b[A\n",
      "Batch:  82%|████████▏ | 164/200 [00:34<00:08,  4.38it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0140, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  82%|████████▎ | 165/200 [00:34<00:08,  4.07it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0140, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  82%|████████▎ | 165/200 [00:35<00:08,  4.07it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0140, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  83%|████████▎ | 166/200 [00:35<00:08,  4.23it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0140, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  83%|████████▎ | 166/200 [00:35<00:08,  4.23it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0140, f1-score=0.344, task=2]\u001b[A\n",
      "Batch:  84%|████████▎ | 167/200 [00:35<00:07,  4.18it/s, batch_loss=0.0075, diff=-.544, epoch_loss=0.0140, f1-score=0.344, task=2]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  84%|████████▎ | 167/200 [00:35<00:07,  4.18it/s, batch_loss=0.0092, diff=0.544, epoch_loss=0.0139, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  84%|████████▍ | 168/200 [00:35<00:07,  4.51it/s, batch_loss=0.0092, diff=0.544, epoch_loss=0.0139, f1-score=0.345, task=1]\u001b[A\n",
      "Batch:  84%|████████▍ | 168/200 [00:35<00:07,  4.51it/s, batch_loss=0.0172, diff=-.118, epoch_loss=0.0139, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  84%|████████▍ | 169/200 [00:35<00:07,  4.43it/s, batch_loss=0.0172, diff=-.118, epoch_loss=0.0139, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  84%|████████▍ | 169/200 [00:35<00:07,  4.43it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.345, task=0]    \u001b[A\n",
      "Batch:  85%|████████▌ | 170/200 [00:35<00:06,  4.82it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  85%|████████▌ | 170/200 [00:36<00:06,  4.82it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  86%|████████▌ | 171/200 [00:36<00:05,  5.14it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  86%|████████▌ | 171/200 [00:36<00:05,  5.14it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  86%|████████▌ | 172/200 [00:36<00:05,  5.47it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  86%|████████▌ | 172/200 [00:36<00:05,  5.47it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0140, f1-score=0.345, task=2]\u001b[A\n",
      "Batch:  86%|████████▋ | 173/200 [00:36<00:04,  5.72it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0140, f1-score=0.345, task=2]\u001b[A\n",
      "Batch:  86%|████████▋ | 173/200 [00:36<00:04,  5.72it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0140, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  87%|████████▋ | 174/200 [00:36<00:04,  6.11it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0140, f1-score=0.345, task=0]\u001b[A\n",
      "Batch:  87%|████████▋ | 174/200 [00:36<00:04,  6.11it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0140, f1-score=0.346, task=1]\u001b[A\n",
      "Batch:  88%|████████▊ | 175/200 [00:36<00:04,  6.14it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0140, f1-score=0.346, task=1]\u001b[A\n",
      "Batch:  88%|████████▊ | 175/200 [00:36<00:04,  6.14it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  88%|████████▊ | 176/200 [00:36<00:03,  6.24it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0140, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  88%|████████▊ | 176/200 [00:36<00:03,  6.24it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.345, task=2]\u001b[A\n",
      "Batch:  88%|████████▊ | 177/200 [00:36<00:03,  6.31it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.345, task=2]\u001b[A\n",
      "Batch:  88%|████████▊ | 177/200 [00:37<00:03,  6.31it/s, batch_loss=0.0092, diff=0.544, epoch_loss=0.0139, f1-score=0.346, task=1]\u001b[A\n",
      "Batch:  89%|████████▉ | 178/200 [00:37<00:03,  6.18it/s, batch_loss=0.0092, diff=0.544, epoch_loss=0.0139, f1-score=0.346, task=1]\u001b[A\n",
      "Batch:  89%|████████▉ | 178/200 [00:37<00:03,  6.18it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0139, f1-score=0.346, task=1]    \u001b[A\n",
      "Batch:  90%|████████▉ | 179/200 [00:37<00:03,  6.12it/s, batch_loss=0.0092, diff=0, epoch_loss=0.0139, f1-score=0.346, task=1]\u001b[A\n",
      "Batch:  90%|████████▉ | 179/200 [00:37<00:03,  6.12it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 180/200 [00:37<00:03,  6.23it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 180/200 [00:37<00:03,  6.23it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.347, task=0]    \u001b[A\n",
      "Batch:  90%|█████████ | 181/200 [00:37<00:03,  6.29it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  90%|█████████ | 181/200 [00:37<00:03,  6.29it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0139, f1-score=0.347, task=1]\u001b[A\n",
      "Batch:  91%|█████████ | 182/200 [00:37<00:03,  4.99it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0139, f1-score=0.347, task=1]\u001b[A\n",
      "Batch:  91%|█████████ | 182/200 [00:38<00:03,  4.99it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 183/200 [00:38<00:03,  4.60it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 183/200 [00:38<00:03,  4.60it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]    \u001b[A\n",
      "Batch:  92%|█████████▏| 184/200 [00:38<00:03,  4.87it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  92%|█████████▏| 184/200 [00:38<00:03,  4.87it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.347, task=2]\u001b[A\n",
      "Batch:  92%|█████████▎| 185/200 [00:38<00:02,  5.31it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.347, task=2]\u001b[A\n",
      "Batch:  92%|█████████▎| 185/200 [00:38<00:02,  5.31it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  93%|█████████▎| 186/200 [00:38<00:02,  5.07it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  93%|█████████▎| 186/200 [00:38<00:02,  5.07it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]    \u001b[A\n",
      "Batch:  94%|█████████▎| 187/200 [00:38<00:02,  5.43it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  94%|█████████▎| 187/200 [00:39<00:02,  5.43it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 188/200 [00:39<00:02,  5.52it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch:  94%|█████████▍| 188/200 [00:39<00:02,  5.52it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.347, task=2]\u001b[A\n",
      "Batch:  94%|█████████▍| 189/200 [00:39<00:01,  5.53it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.347, task=2]\u001b[A\n",
      "Batch:  94%|█████████▍| 189/200 [00:39<00:01,  5.53it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0139, f1-score=0.346, task=2]    \u001b[A\n",
      "Batch:  95%|█████████▌| 190/200 [00:39<00:01,  5.05it/s, batch_loss=0.0075, diff=0, epoch_loss=0.0139, f1-score=0.346, task=2]\u001b[A\n",
      "Batch:  95%|█████████▌| 190/200 [00:39<00:01,  5.05it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  96%|█████████▌| 191/200 [00:39<00:01,  4.65it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  96%|█████████▌| 191/200 [00:39<00:01,  4.65it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.346, task=2]\u001b[A\n",
      "Batch:  96%|█████████▌| 192/200 [00:39<00:01,  5.21it/s, batch_loss=0.0075, diff=-.426, epoch_loss=0.0139, f1-score=0.346, task=2]\u001b[A\n",
      "Batch:  96%|█████████▌| 192/200 [00:40<00:01,  5.21it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  96%|█████████▋| 193/200 [00:40<00:01,  5.30it/s, batch_loss=0.0171, diff=0.426, epoch_loss=0.0139, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  96%|█████████▋| 193/200 [00:40<00:01,  5.30it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0139, f1-score=0.346, task=0]    \u001b[A\n",
      "Batch:  97%|█████████▋| 194/200 [00:40<00:01,  5.58it/s, batch_loss=0.0172, diff=0, epoch_loss=0.0139, f1-score=0.346, task=0]\u001b[A\n",
      "Batch:  97%|█████████▋| 194/200 [00:40<00:01,  5.58it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0139, f1-score=0.347, task=1]\u001b[A\n",
      "Batch:  98%|█████████▊| 195/200 [00:40<00:00,  5.18it/s, batch_loss=0.0092, diff=0.118, epoch_loss=0.0139, f1-score=0.347, task=1]\u001b[A\n",
      "Batch:  98%|█████████▊| 195/200 [00:40<00:00,  5.18it/s, batch_loss=0.0091, diff=0, epoch_loss=0.0138, f1-score=0.347, task=1]    \u001b[A\n",
      "Batch:  98%|█████████▊| 196/200 [00:40<00:00,  5.40it/s, batch_loss=0.0091, diff=0, epoch_loss=0.0138, f1-score=0.347, task=1]\u001b[A\n",
      "Batch:  98%|█████████▊| 196/200 [00:40<00:00,  5.40it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  98%|█████████▊| 197/200 [00:40<00:00,  4.55it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.347, task=0]\u001b[A\n",
      "Batch:  98%|█████████▊| 197/200 [00:41<00:00,  4.55it/s, batch_loss=0.0091, diff=0.118, epoch_loss=0.0138, f1-score=0.348, task=1]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  99%|█████████▉| 198/200 [00:41<00:00,  4.99it/s, batch_loss=0.0091, diff=0.118, epoch_loss=0.0138, f1-score=0.348, task=1]\u001b[A\n",
      "Batch:  99%|█████████▉| 198/200 [00:41<00:00,  4.99it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch: 100%|█████████▉| 199/200 [00:41<00:00,  5.28it/s, batch_loss=0.0171, diff=-.118, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Batch: 100%|█████████▉| 199/200 [00:41<00:00,  5.28it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]    \u001b[A\n",
      "Batch: 100%|██████████| 200/200 [00:41<00:00,  5.54it/s, batch_loss=0.0171, diff=0, epoch_loss=0.0139, f1-score=0.348, task=0]\u001b[A\n",
      "Training model: 100%|██████████| 2/2 [01:19<00:00, 39.86s/it, epoch_loss=0.0139]                                              \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n",
      "Exception occurred: Labels have already been processed. Dataset name: Semeval (2016)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.014845838099718093, 0.013874369596596808], [], None, [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohmlp = OnehotMLPClassifier(input_dims = input_dims, shared_dim = shared_dim, hidden_dims = hidden_dims,\n",
    "                            output_dims = output_dims, dropout = dropout)\n",
    "optim = opt.SGD(ohmlp.parameters(), lr = 0.01)\n",
    "loss = CrossEntropyLoss()\n",
    "m = Metrics(['f1-score'], display_metric = 'f1-score', early_stop = 'f1-score')\n",
    "train_mtl_model(ohmlp, training_datasets = [sent, off, mftc], save_path = '.', opt = optim, loss_func = loss,\n",
    "                metrics = m, batch_size = 32, epochs = 2, clip = 1.0, patience = 3, dev = sent, dev_task_id = 0,\n",
    "                batches_per_epoch = 200, shuffle_data = True, dataset_weights = [0.6, 0.2, 0.2],\n",
    "                loss_weights = [0.6, 0.3, 0.1], onehot = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ohlstm = OnehotLSTMClassifier(input_dims = input_dims, shared_dim = shared_dim, hidden_dims = hidden_dims, \n",
    "                              output_dims = output_dims, dropout = dropout, no_layers = 1)\n",
    "ohlstm_optim = opt.SGD(ohlstm.parameters(), lr = 0.01)\n",
    "loss = CrossEntropyLoss()\n",
    "m = Metrics(['accuracy', 'f1'], display_metric = 'accuracy', early_stop = 'accuracy')\n",
    "train_mtl_model(ohlstm, training_datasets = [sent, off, mftc], save_path = '.', opt = ohlstm_optim, loss_func = loss,\n",
    "                metrics = m, batch_size = 32, epochs = 5, clip = 1.0, patience = 3, dev = sent, dev_task_id = 0,\n",
    "                batches_per_epoch = None, shuffle_data = True, dataset_weights = [0.6, 0.2, 0.2],\n",
    "                loss_weights = [0.6, 0.3, 0.1], onehot = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emblstm = EmbeddingLSTMClassifier(input_dims = input_dims, embedding_dim = embedding_dim, shared_dim = shared_dim,\n",
    "                                  hidden_dims = hidden_dims, output_dims = output_dims, dropout = dropout, \n",
    "                                  no_layers = 1)\n",
    "optim = opt.SGD(emblstm.parameters(), lr = 0.01)\n",
    "loss = CrossEntropyLoss()\n",
    "m = Metrics(['accuracy', 'f1'], display_metric = 'accuracy', early_stop = 'accuracy')\n",
    "train_mtl_model(emblstm, training_datasets = [sent, off, mftc], save_path = '.', opt = optim, loss_func = loss,\n",
    "                metrics = m, batch_size = 32, epochs = 5, clip = 1.0, patience = 3, dev = sent, dev_task_id = 0,\n",
    "                batches_per_epoch = None, shuffle_data = True, dataset_weights = [0.6, 0.2, 0.2],\n",
    "                loss_weights = [0.6, 0.3, 0.1], onehot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
