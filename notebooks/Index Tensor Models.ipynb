{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import mlearn.base as base\n",
    "from mlearn.base import Field\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlearn.utils.pipeline import process_and_batch\n",
    "from mlearn.data_processing.data import GeneralDataset\n",
    "from mlearn.modeling.embedding import EmbeddingRNNClassifier, EmbeddingLSTMClassifier, CNNClassifier, EmbeddingMLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Test (train): 1914it [00:00, 26759.84it/s]\n",
      "Loading Test (test): 478it [00:00, 24797.80it/s]\n"
     ]
    }
   ],
   "source": [
    "text_field = Field('text', train = True, label = False, ignore = False, ix = 5, cname = 'text')\n",
    "label_field = Field('label', train = False, label = True, cname = 'label', ignore = False, ix = 4)\n",
    "ignore_field = Field('ignore', train = False, label = False, cname = 'ignore', ignore = True)\n",
    "\n",
    "fields = [text_field, label_field]\n",
    "dataset = GeneralDataset(data_dir = '~/PhD/projects/tools/mlearn/tests/',\n",
    "                         ftype = 'csv', fields = fields, train = 'garcia_stormfront_train.tsv', dev = None,\n",
    "                         test = 'garcia_stormfront_test.tsv', train_labels = None, tokenizer = lambda x: x.split(),\n",
    "                         lower = True, preprocessor = None, transformations = None,\n",
    "                         label_processor = None, sep = '\\t', name = 'Test')\n",
    "dataset.load('train')\n",
    "dataset.load('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the documents and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1914/1914 [00:00<00:00, 128214.55it/s]\n",
      "Encoding vocabulary: 100%|██████████| 6291/6291 [00:00<00:00, 494108.21it/s]\n",
      "Encode label vocab: 100%|██████████| 2/2 [00:00<00:00, 1766.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.build_token_vocab(dataset.data)\n",
    "dataset.build_label_vocab(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_and_batch(dataset, dataset.data, 32, onehot = False)\n",
    "test = process_and_batch(dataset, dataset.test, 32, onehot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 60\n",
      "Length of first batch: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches:\", len(train))\n",
    "print(\"Length of first batch:\", len(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingRNNClassifier(dataset.vocab_size(), hidden_dim = 128, embedding_dim = dataset.vocab_size(), output_dim = 3, batch_first = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [04:26<00:00,  4.44s/it, accuracy=0, loss=0.125]  \n"
     ]
    }
   ],
   "source": [
    "with tqdm(train) as loop:\n",
    "    for X, y in loop:\n",
    "        res = model(X.long())\n",
    "        l = loss(res, y)\n",
    "        \n",
    "        acc = accuracy_score(res.argmax(dim=1).cpu(), y.cpu())\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss = l.data.item() / X.shape[0], accuracy = acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingLSTMClassifier(dataset.vocab_size(), hidden_dim = 128, embedding_dim = 128, output_dim = 3, num_layers = 1, batch_first = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [03:09<00:00,  3.16s/it, accuracy=0.308, loss=0.126]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(train) as loop:\n",
    "    for X, y in loop:\n",
    "        res = model(X.long())\n",
    "        l = loss(res, y)\n",
    "        \n",
    "        acc = accuracy_score(res.argmax(dim=1).cpu(), y.cpu())\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss = l.data.item() / X.shape[0], accuracy = acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingMLPClassifier(dataset.vocab_size(), hidden_dim = 128, embedding_dim = 128, output_dim = 3, batch_first = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:11<00:00,  5.22it/s, accuracy=0.5, loss=1]       \n"
     ]
    }
   ],
   "source": [
    "with tqdm(train) as loop:\n",
    "    for X, y in loop:\n",
    "        res = model(X.long())\n",
    "        l = loss(res, y)\n",
    "        \n",
    "        acc = accuracy_score(res.argmax(dim=1).cpu(), y.cpu())\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss = l.data.item() / X.shape[0], accuracy = acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"CNN Classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, window_sizes: base.List[int], num_filters: int, max_feats: int, input_dim: int, \n",
    "                 embedding_dim: int, output_dim: int, batch_first: bool = True, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the model.\n",
    "\n",
    "        :window_sizes: The size of the filters (e.g. 1: unigram, 2: bigram, etc.)\n",
    "        :no_filters: The number of filters to apply.\n",
    "        :max_feats: The maximum length of the sequence to consider.\n",
    "        :hidden_dim (int): Hidden dimension size.\n",
    "        :output_dim (int): Output dimension.\n",
    "        :batch_first (bool, default: True): True if the batch is the first dimension.\n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.name = 'cnn'\n",
    "\n",
    "        self.itoh = nn.Embedding(input_dim, embedding_dim)  # Works\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, num_filters, (w, embedding_dim)) for w in window_sizes])\n",
    "        self.linear = nn.Linear(len(window_sizes) * num_filters, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, sequence) -> base.DataType:\n",
    "        \"\"\"\n",
    "        Forward step of the model.\n",
    "\n",
    "        :sequence: The sequence to be predicted on.\n",
    "        :return (base.DataType): The scores computed by the model.\n",
    "        \"\"\"\n",
    "        # CNNs expect batch first so let's try that\n",
    "        if not self.batch_first:\n",
    "            sequence = sequence.transpose(0, 1)\n",
    "\n",
    "        emb = self.itoh(sequence)  # Get embeddings for sequence\n",
    "        output = [F.relu(conv(emb.unsqueeze(1))).squeeze(3) for conv in self.conv]\n",
    "        output = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in output]\n",
    "        output = torch.cat(output, 1)\n",
    "        scores = self.softmax(self.linear(output))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(window_sizes = [2,3,4], num_filters = 128, max_feats = 100, \n",
    "                      output_dim = 3, vocab_size = dataset.vocab_size(), embedding_dim = 128,\n",
    "                      batch_first = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:20<00:00,  2.89it/s, accuracy=0.5, loss=0.854]   \n"
     ]
    }
   ],
   "source": [
    "with tqdm(train) as loop:\n",
    "    for X, y in loop:\n",
    "        res = model(X.long())\n",
    "        l = loss(res, y)\n",
    "        \n",
    "        acc = accuracy_score(res.argmax(dim=1).cpu(), y.cpu())\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss = l.data.item() / X.shape[0], accuracy = acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
